{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOsurVv1t1wu"
      },
      "outputs": [],
      "source": [
        "test_doc_id = 'ENCPOS_2002_29'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3XT8vELcCBd"
      },
      "source": [
        "# Loading the dataset (`docs_structured`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BWmUsOQKd1T"
      },
      "outputs": [],
      "source": [
        "#download, unzip and reading encpos dataset\n",
        "import glob\n",
        "\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/data/encpos_txt.zip\n",
        "#!wget https://github.com/chartes/encpos_similarities/raw/master/data/encpos_sample_txt.zip\n",
        "!unzip encpos_txt.zip -d /content/\n",
        "\n",
        "docs = [] # list of documents (a single line string for each doc)\n",
        "docs=[open(filename, \"r\").readlines() for filename in glob.glob(\"/content/encpos_txt/*.txt\")]\n",
        "docs=[\" \".join(x).replace(\"\\n\", \"\") for x in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlcdckuHmArs"
      },
      "outputs": [],
      "source": [
        "# Extracting chapters structuration from texts\n",
        "\n",
        "import re\n",
        "\n",
        "docs_structured={} # dict of docs to process ('metadata' + one item for each text div)\n",
        "for index, doc in enumerate(docs):\n",
        "  chapters=re.findall(r\"==.+?\\==\" , doc) #match chapters (and subchaters) titles as the pattern is \"===\" and \"==\"\n",
        "  regexPattern = '|'.join(map(re.escape, chapters))\n",
        "  a=re.split(regexPattern, doc)\n",
        "\n",
        "  text_structured={x:y for x,y in list(zip([\"metadata\"]+chapters, a))}\n",
        "  try:\n",
        "    identifier=text_structured[\"metadata\"].split(\"identifier: \")[1].split(\" \", 1)[0].replace(\" \", \"\")\n",
        "    docs_structured[identifier]=text_structured\n",
        "  except:\n",
        "    #print(text_structured[\"metadata\"])\n",
        "    identifier=doc.split(\"identifier: \")[1].split(\" \",1)[0]\n",
        "    resume=doc.split(\"title: \")[1]\n",
        "    docs_structured[identifier]={\"title\":resume}\n",
        "    #print(texts[index], \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "39TpbPeI_OHe",
        "outputId": "2960135e-6beb-4332-af99-0a1ebca95157"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  L’abbaye de Fontenay, grandie sous le regard de saint Bernard, s’est dotée, par une activité de copie d’abord externe, puis dans l’abbaye même, d’une bibliothèque répondant aux exigences ascétiques et spirituelles de l’abbé de Clairvaux. Mais l’austérité polychrome a été remplacée par un monochromatisme splendide qui respectait plus la lettre que l’esprit de pauvreté tel qu’il a été formulé par saint Bernard. Le temps de la production des livres est assez bref et celui des réalisations monochromes encore plus. La production importante dans la seconde moitié du xiie siècle est poursuivie par un enrichissement sporadique fondé sur la production laïque, les dons des moines et leurs acquisitions à Paris. L’abbaye de Fontenay comptait probablement cent soixante-dix à deux cents manuscrits au xiiie siècle et près de cinq cents volumes à la fin du xve siècle ou au début du xvie siècle. Malgré une importance tout à fait honorable, l’abbaye ne semble pourtant pas avoir eu de rôle majeur dans l’établissement des textes et des collections de textes. En revanche, la bibliothèque de Fontenay tient une place inattendue dans les réseaux de transmission des textes. Elle est certes alimentée principalement par des modèles de Clairvaux, mais elle est plusieurs fois plus proche de Cîteaux que de Clairvaux et assure notamment la liaison entre Paris et l’abbaye chef d’ordre pour les œuvres de Hugues de Saint-Victor. L’existence d’un réseau local pour le choix des textes est difficilement envisageable, car il y a peu de communautés régulières autour de Montbard qui possèdent des bibliothèques reconnues. En revanche, Fontenay est proche d’une route importante et sa situation géographique favorise l’action des influences extérieures. La bibliothèque de Fontenay a également servi de conservatoire pour quelques textes inédits ou rares, comme les collections chrysostomiennes, le poème figuré de Raban Maur In honorem sanctae Crucis, le commentaire de Jacques de Thérines sur l’Apocalypse, le commentaire de Gui de l’Aumône sur les Sentences et une version abrégée des Quodlibets de Henri de Gand ; le catalogue partiel du xviiie siècle atteste aussi la présence de la collection épistolaire pseudo-ignacienne, des Antiquités judaïques de Flavius Josèphe et des Moralia in Job, œuvre si richement décorée à Cîteaux, à Clairvaux et à La Ferté. Si les moines n’ont pas laissé leur nom dans l’histoire littéraire ou la vie spirituelle les abbés étaient trop engagés dans les affaires du siècle pour promouvoir l’abbaye dans un contexte d’affaiblissement général de l’Ordre , la bibliothèque n’est pas négligée et est restée, selon les mots du Chapitre général, le “ trésor des moines ”. Au milieu des luttes permanentes entre les abbés et la communauté, les manuscrits perdent de leur importance au profit des archives dont l’intérêt historique a été reconnu assez tôt , ainsi qu’au profit des imprimés qui ouvrent l’abbaye à la culture des Lumières et aux sciences.   '"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "docs_structured[test_doc_id]['metadata']\n",
        "docs_structured[test_doc_id]['== Conclusion ==']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqZJyN7NFKSE"
      },
      "source": [
        "# Corpus preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYNyne3cJQnw"
      },
      "source": [
        "## Sentences segmentation (`docs_structured_sents`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ha93vK3qfP"
      },
      "source": [
        "#### 1. Télécharger le corpus segmenté en phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTS-GMpf194I",
        "outputId": "898d7f88-fc3e-45b8-9cde-567612da8cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-20 11:57:54--  https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_structured_sents.json\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/chartes/encpos_similarities/master/data_structured/encpos_structured_sents.json [following]\n",
            "--2022-07-20 11:57:54--  https://raw.githubusercontent.com/chartes/encpos_similarities/master/data_structured/encpos_structured_sents.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49706859 (47M) [text/plain]\n",
            "Saving to: ‘encpos_structured_sents.json’\n",
            "\n",
            "encpos_structured_s 100%[===================>]  47.40M   204MB/s    in 0.2s    \n",
            "\n",
            "2022-07-20 11:57:55 (204 MB/s) - ‘encpos_structured_sents.json’ saved [49706859/49706859]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_structured_sents.json\n",
        "\n",
        "docs_structured_sents = {}  # dict of docs (with their 'metadata' + one item for each text div with a list of its sentences)\n",
        "\n",
        "with open('/content/encpos_structured_sents.json') as json_file:\n",
        "  docs_structured_sents = json.load(json_file)\n",
        "\n",
        "# docs_structured_sents.keys()\n",
        "# docs_structured_sents['ENCPOS_2002_29']['metadata']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYcMl2kVAEZK"
      },
      "source": [
        "#### 2. (OU) Segmenter le corpus en phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_C-oGPOrQmd",
        "outputId": "228f294f-86f7-480d-8e13-f58b73d5fe57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "#Option 2: extracting chapter structuration with sentences split (it can take a while)\n",
        "\n",
        "# Spacy required for senteces segmentation\n",
        "!pip install -U spacy[cuda92,transformers,lookups] # tout est requis ici ?\n",
        "!python -m spacy download fr_core_news_lg\n",
        "import spacy\n",
        "nlp=spacy.load(\"fr_core_news_lg\") #spacy linguistic model for french news large\n",
        "\n",
        "docs_structured_sents={} # dict of docs ('metadata' + one item for each text div with a list of its sentences)\n",
        "for index, doc in enumerate(docs):\n",
        "  chapters=re.findall(r\"==.+?\\==\" , doc) #chapters and subchaters as the pattern is \"===\" and \"==\"\n",
        "  regexPattern = '|'.join(map(re.escape, chapters))\n",
        "  a=re.split(regexPattern, doc)\n",
        "\n",
        "  text_structured={x:y for x,y in list(zip([\"metadata\"]+chapters, a))}\n",
        "  try:\n",
        "    identifier=text_structured[\"metadata\"].split(\"identifier: \")[1].split(\" \", 1)[0].replace(\" \", \"\")\n",
        "    text_structured={k:[sent.text for sent in nlp(v).sents] for k,v in text_structured.items()}\n",
        "    docs_structured_sents[identifier]=text_structured\n",
        "  except:\n",
        "    #print(text_structured[\"metadata\"])\n",
        "    identifier=doc.split(\"identifier: \")[1].split(\" \",1)[0]\n",
        "    #resume=doc.split(\"title: \")[1]\n",
        "    docs_structured_sents[identifier]={\"metadata\":[sent.text for sent in nlp(doc).sents]}\n",
        "  if index%400==0: #there are >2900 positions\n",
        "    print(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg9IWPMQ6eWr"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "docs_structured_sents[test_doc_id]['metadata']\n",
        "docs_structured_sents[test_doc_id]['== Conclusion ==']\n",
        "# docs_structured_sents[test_doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFiBD61IwRL5"
      },
      "outputs": [],
      "source": [
        "# Export `docs_structured_sents`\n",
        "\n",
        "#JSON or pickle dump of docs_structured_sents (dict with chapters and sentences)\n",
        "import json\n",
        "with open('encpos_structured_sents.json', \"w\", encoding='utf8') as f:\n",
        "    json.dump(docs_structured_sents, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwgN-VTeEbbN"
      },
      "source": [
        "## Keywords extraction (`keywords_by_doc`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Z_vW3JEfBR"
      },
      "source": [
        "### Télécharger la liste des keywords par doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9vMbKEHRLAj",
        "outputId": "2b637aa9-c296-42de-88b2-462029b94ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-20 11:58:02--  https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_keywords_by_doc.json\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/chartes/encpos_similarities/master/data_structured/encpos_keywords_by_doc.json [following]\n",
            "--2022-07-20 11:58:02--  https://raw.githubusercontent.com/chartes/encpos_similarities/master/data_structured/encpos_keywords_by_doc.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2256103 (2.2M) [text/plain]\n",
            "Saving to: ‘encpos_keywords_by_doc.json’\n",
            "\n",
            "encpos_keywords_by_ 100%[===================>]   2.15M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-07-20 11:58:03 (33.3 MB/s) - ‘encpos_keywords_by_doc.json’ saved [2256103/2256103]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_keywords_by_doc.json\n",
        "keywords_by_doc = {}        # dict of docs with their discriminant keywords\n",
        "with open('/content/encpos_keywords_by_doc.json') as json_file:\n",
        "  keywords_by_doc = json.load(json_file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "BrzZduqy1ETX",
        "outputId": "7edcf5ac-53f7-4b65-c826-c3f569ccf10a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'saint cisterciennes fontenay bibliothèques clairvaux pontigny abbaye inventaires documents manuscrits catalogue sources médiéval bibliothèque paris signatures cahier parchemin support peaux écriture manuscrits clairvaux littérature bibliothèque manuscrits bibliques liturgiques fontenay abbaye cloître emplacement sacristie claustri dépôts abbaye armarium bibliothèque bois seules recouverts abbaye huit siècle manuscrits médiévales libris livres fonds biblique bible bibliothèque bibliothécaire abbaye contenue liste moine médiévale bibliothèque abbaye manuscrit fontenay léon siècle libris apost lettres bibliothèque abbaye vatican françois pierre bibliothèque généalogie textes fontenay manuscrits abbaye bourgogne bibliothèque baptiste bibliothécaire manuscrits colbert abbé abbaye 1650 œuvres bibl saint fontenay baluze abbaye manuscrit liste bourgogne église témoignage livre littéraire ouvrages manuscrits bouhier font rédigée auteur bibliothèque catalogue liste manuscrits manuscrits bibliothèque 1774 bibliothécaire bibliophile fontenay abbaye abbé 1802 fontenay bibliothèque parisien libraire abbaye manuscrit bibliothèques saint bernard fontenay abbés abbaye edition auteurs table libris catalogue manuscrits '"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords_by_doc[test_doc_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXi3zyOjEl9Y"
      },
      "source": [
        "### (OU) Calculer les keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PTKvvSjZQuQ",
        "outputId": "6c4dafda-9057-4e05-cab0-833d84ed544f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.0+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.20.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.0+cu113)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "#Loading extraction functions\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Bert\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased-v1') #multilingual model for keywork extraction, text summarization and sentence transformation (0.7 PR)\n",
        "\n",
        "# nltk for stopwords and punct? Only HERE ?\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('french')\n",
        "\n",
        "!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# test, régler fuzz\n",
        "# fuzz.ratio('bibliothécaires', 'bibliothèques')\n",
        "\n",
        "\"\"\"\n",
        "The idea behind is to compare the full paragraph embbeding against all possible 8-words combination embeddings (candidates)\n",
        "Then, we select the 8-words closest (by cosine similarity) to the full paragraph embeddings, as they are our more representative keywords.  \n",
        "\"\"\"\n",
        "def key_extractor(doc, top_n=8, n_gram_range = (1, 1)): #function to extract keywords from a text, n_gram_range indicates the matrice range of candidates\n",
        "  vectorizer = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
        "\n",
        "  candidates = vectorizer.get_feature_names_out() \n",
        "  candidate_embeddings = model.encode(candidates) # fréquence des mots plus important que leur ordre (embedding de la fréquence du lexique dans chaque bloc) ? \n",
        "\n",
        "  doc_embedding = model.encode([doc])\n",
        "\n",
        "  distances = cosine_similarity(doc_embedding, candidate_embeddings) # expliquer\n",
        "  keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "\n",
        "  #optional to filter similar keywords inside the same group (vg. Bibliothèque, bibliothècaire)\n",
        "  keywords_fuzzy=[]\n",
        "  for i, x in enumerate(keywords):\n",
        "    if i>0:\n",
        "      if any(fuzz.ratio(y,x)>85 for y in keywords_fuzzy):\n",
        "        continue\n",
        "      else:\n",
        "        keywords_fuzzy.append(x)\n",
        "    else:\n",
        "      keywords_fuzzy.append(x)\n",
        "\n",
        "  return keywords_fuzzy\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TODO: filtrer way de filtrer les keywords trop proches – cf fuzzy method de key_extractor(), en mieux\n",
        "\"\"\"\n",
        "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):#extraction by cosine embeddings similarity\n",
        "    # Calculate distances and extract keywords\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
        "                                            candidate_embeddings)\n",
        "\n",
        "    # Get top_n words as candidates based on cosine similarity\n",
        "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "    words_vals = [candidates[index] for index in words_idx]\n",
        "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "    # Calculate the combination of words that are the least similar to each other\n",
        "    min_sim = np.inf\n",
        "    candidate = None\n",
        "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "        if sim < min_sim:\n",
        "            candidate = combination\n",
        "            min_sim = sim\n",
        "\n",
        "    return [words_vals[idx] for idx in candidate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KKiawI2TThl"
      },
      "outputs": [],
      "source": [
        "# DEBUG VJ pour keywords\n",
        "# store keywords vector for each doc\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "count=0\n",
        "keywords_by_doc = {} # stocker les mots clé de chaque doc\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  # print(doc_id)\n",
        "  keyword_bloc=\"\"\n",
        "  for k,v in docs_structured[doc_id].items():\n",
        "    if k!=\"metadata\":\n",
        "      if len(v)>10: # des chapitres avec uniquement des sauts de lignes…\n",
        "        try:\n",
        "          keyword_bloc+=\" \".join(key_extractor(v))+\" \"\n",
        "        except:\n",
        "          continue\n",
        "  keywords_by_doc[doc_id] = keyword_bloc\n",
        "\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wh8YDGvPwB-"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "#JSON dump of keywords_by_doc (dict with chapters and sentences)\n",
        "import json\n",
        "with open('encpos_keywords_by_doc.json', \"w\", encoding='utf8') as f:\n",
        "    json.dump(keywords_by_doc, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dH1Uk5ZQ7-f"
      },
      "outputs": [],
      "source": [
        "# TEST SERGIO // SUPPRIMER ???\n",
        "#keyword extraction for the test position\n",
        "#apply key_extractor on each text div (chapter)\n",
        "for k,v in docs_structured_sents[test_doc_id].items():\n",
        "  bloc=\" \".join(v)\n",
        "  if len(bloc)>10:\n",
        "    print(\"\\t\", k,\"\\n\")\n",
        "    bloc=\" \".join(v)\n",
        "    #print(\"\\t\", bloc)\n",
        "    print(\"\\t\", key_extractor(bloc), \"\\n\\n\")\n",
        "  else:\n",
        "    print(k,\"\\n\")\n",
        "    print(bloc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZXIGkOUaV7A"
      },
      "source": [
        "## Entities extraction (`entities_by_doc`)\n",
        "\n",
        "TODO. Expliquer pourquoi on travaille au niveau de la phrase (`docs_structured_sents`)\n",
        "\n",
        "Long à calculer. On peut :\n",
        "\n",
        "1. Télécharger la liste déjà calculée\n",
        "1. Calculer la *liste*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idON8EX-Dl-y"
      },
      "source": [
        "#### 1. Télécharger la liste des entités par doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuAHzn5tDRZP",
        "outputId": "9104c1d3-2f1d-433d-81f3-09f86e46e57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-20 11:58:07--  https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_entities_by_doc.json\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/chartes/encpos_similarities/master/data_structured/encpos_entities_by_doc.json [following]\n",
            "--2022-07-20 11:58:08--  https://raw.githubusercontent.com/chartes/encpos_similarities/master/data_structured/encpos_entities_by_doc.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309295 (1.2M) [text/plain]\n",
            "Saving to: ‘encpos_entities_by_doc.json’\n",
            "\n",
            "encpos_entities_by_ 100%[===================>]   1.25M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-07-20 11:58:08 (24.4 MB/s) - ‘encpos_entities_by_doc.json’ saved [1309295/1309295]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_entities_by_doc.json\n",
        "entities_by_doc = {}        # dict of docs with their discriminant entities\n",
        "with open('/content/encpos_entities_by_doc.json') as json_file:\n",
        "  entities_by_doc = json.load(json_file) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftClGbX91x6x",
        "outputId": "9e0dce1c-c6d7-464a-c144-69e1bd52cf80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Clairvaux',\n",
              " 'Paris',\n",
              " 'Fontenay',\n",
              " 'Cîteaux',\n",
              " 'La Ferté',\n",
              " 'saint Bernard',\n",
              " 'Yves de Chartres',\n",
              " 'saint Ambroise',\n",
              " 'bibliothèque de Fontenay',\n",
              " 'Baluze',\n",
              " 'Colbert',\n",
              " 'saint Jérôme',\n",
              " 'abbaye de Fontenay',\n",
              " 'Apocalypse',\n",
              " 'Geoffroy d’Auxerre',\n",
              " 'Basile de Césarée',\n",
              " 'Hugues de Saint-Victor',\n",
              " 'Bouchu',\n",
              " 'bibliothèque de Colbert',\n",
              " 'Raban',\n",
              " 'sanctae Crucis']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entities_by_doc[test_doc_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Crz0LED3-N"
      },
      "source": [
        "#### 2. (OU) Calculer les entités discriminantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9IXVNufjkuF"
      },
      "outputs": [],
      "source": [
        "#installing spacy\n",
        "!pip install -U pip setuptools wheel # ?\n",
        "!pip install -U spacy[cuda92,transformers,lookups]\n",
        "import spacy\n",
        "\n",
        "#Download language model for modern french. If you get a downloading error you must restart the runtime\n",
        "!python -m spacy download fr_core_news_lg\n",
        "nlp=spacy.load(\"fr_core_news_lg\") #spacy linguistic model for french news large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za5ymAPA4VAn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "function to extract meaning relationships between entities based on (Spacy) dependencies\n",
        "TODO\n",
        "text = doc ? ou sentence ?\n",
        "\"\"\"\n",
        "def rel_extraction(text):\n",
        "  keys=[\"ROOT\", \"nsubj\"]\n",
        "  keys_2=[\"LOC\", \"PER\", \"ORG\"]\n",
        "\n",
        "  doc_dep = nlp(text)\n",
        "  doc_dep=[[tok.text, tok.dep_] for tok in doc_dep]\n",
        "\n",
        "  doc_ents= spacy_large_ner(text, nlp)\n",
        "  doct_ents=list(doc_ents)\n",
        "  doc_rel=[\"O\"]*len(doc_dep)\n",
        "  for ent in list(doc_ents): doc_rel[ent[-2]:ent[-1]]=[ent[-3]]*(ent[-1]-ent[-2])\n",
        "\n",
        "  #doc_merged=[\"\\t\".join(x+[y]) for x,y in list(zip(doc_dep, doc_rel))]\n",
        "  doc_merged=[x+[y] for x,y in list(zip(doc_dep, doc_rel))]\n",
        "\n",
        "  doc_merged=[x[0] for x in doc_merged if x[1] in keys or x[2] in keys_2 ]\n",
        "\n",
        "  return doc_merged\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "extract entities and character indexes\n",
        "TODO\n",
        "\"\"\"\n",
        "def spacy_large_ner(document, model):\n",
        "  return {(ent.text.strip(), ent.label_, ent.start, ent.end) for ent in model(document).ents}\n",
        "\n",
        "\"\"\"\n",
        "extract entities\n",
        "TODO\n",
        "\"\"\"\n",
        "def spacy_short_ner(document, model): #extract just entities text\n",
        "  return [ent.text.strip() for ent in model(document).ents]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41Q-G_uEWZ71",
        "outputId": "fc68ff42-c31c-4cf9-d68d-fd6c8df0b998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENCPOS_1971_12\n",
            "\t ['CC 12', 'États du Mâçonnais', 'Dijon']\n",
            "ENCPOS_1971_25\n",
            "\t ['Johannes Mülberg', 'Beginarum', 'Lolhardorum', 'Positio pro', 'Beginarum du Mineur', 'Rudolphe Buchsmann', 'Materia', 'Constance Félix Hemmerlin', 'Sébastien Brant', 'De valido mendicante', 'Nota contra', 'Lolhardos', 'Beghardos ac alios', 'dicunt res', 'Strasbourg', 'Constance', 'Predicatores N 5', 'Staatsarchiv', 'Kantons Basel-Stadt', 'Beginenstreit', 'Oddon de Colonna', 'Jacobinus de Torso', 'Cologne', 'Mayence']\n",
            "ENCPOS_1971_15\n",
            "\t ['Jacques Despars', 'Faculté de médecine de Paris', 'Avicenne', 'Canon', 'Antidotarium', 'Guillaume Bernard', 'Galien', 'Canon d’', 'Tournai', 'Bibliothèque Nationale', 'I, fen 1', 'Albert le Grand', 'Johannitius', 'Isaac', 'Jean Damascène', 'Dictionnaire', 'France', 'Ernest Wickersheimer', 'Eudes de Creil', 'Guillaume', 'M. Jacques Monfrin', 'Synonyma', 'Simon de Gênes', 'Arabes']\n",
            "ENCPOS_2002_29\n",
            "\t ['Clairvaux', 'Fontenay', 'Cîteaux', 'Paris', 'La Ferté', 'Yves de Chartres', 'Colbert', 'saint Jérôme', 'saint Bernard', 'Apocalypse', 'Geoffroy d’Auxerre', 'Basile de Césarée', 'Isaïe', 'Ambroise', 'bibliothèque de Fontenay', 'abbaye de Fontenay', 'Arsenal', 'Hugues de Saint-Victor', 'Bouchu', 'bibliothèque de Colbert', 'Pontigny', 'Troyes', 'Bourgogne']\n",
            "ENCPOS_2003_19\n",
            "\t ['Nogaret', 'Montpellier', 'roi de France', 'roi de Majorque', 'Guillaume de Nogaret', 'Languedoc', 'loi du Midi', 'Maguelonne', 'Paris', 'Trésor des chartes', 'Boniface VIII', 'Philippe le Bel', 'J des Archives nationales', 'Trésor', 'Champagne', 'Bernard Saisset', 'Temple', 'Guichard de Troyes', 'saint Louis', 'Alphonse de Poitiers', 'Lunel', 'Anagni', 'Layettes', 'Beaucaire-Nîmes', 'Parlement']\n",
            "ENCPOS_1999_35\n",
            "\t ['Ars lectoria', 'Jean de Garlande', 'Dictionarius', 'Bruges', 'Ars lectoria Ecclesie', 'Isidore', 'Doctrinale', 'Angleterre', 'Priscien', 'Virgile', 'Ovide', 'Lucain', 'Juvénal', 'Perse', 'Toulouse', 'Paris', 'Lincoln', 'Bayeux', 'Catalogue général des manuscrits des bibliothèques publiques de France', 'abbaye des Dunes', 'Clairvaux', 'Reichling']\n",
            "ENCPOS_1972_18\n",
            "\t ['Écosse', 'Angleterre', 'Italie', 'Allemagne', 'France', 'Suisse', 'Espagne', 'Scandinavie', 'Europe', 'Plats-Pays', 'Hollande', 'l’Empire', 'Pologne', 'Brabant', 'l’Europe', 'Hongrie', 'Bibliothèque nationale', 'Savoie', 'Flandre', 'la France', 'mer du Nord', 'Baltique', 'Franconie', 'Autriche']\n",
            "ENCPOS_2000_08\n",
            "\t ['Conseil', 'Philippe V', 'Louis X', 'Chambre des', 'Charles IV', 'Martin des Essarts', 'Hôtel', 'Parlement', 'N. Valois', 'P. Lehugeur', 'Philippe de Poitiers', 'de France', 'Philippe VI', 'Comptes, sans doute', 'Noster1', 'Pater,', 'Qui es in ccelis,', 'JJ 57', 'Archives nationales', 'Conseil du roi', 'Chambre', 'Paul Lehugeur', 'Henri de Sully', 'Conseils', 'Pontoise']\n",
            "ENCPOS_1994_04\n",
            "\t ['Yolande', 'Barrois', 'Temple', 'Bibliothèque nationale', 'Lorraine', 'Yolande, régente', 'Moreau', 'Dupuy', 'Clairambault', 'Mélanges Colbert', 'J et X des Archives nationales', 'Archives départementales de', 'Meurthe-et-Moselle', 'Nancy', 'Archives départementales de la Meuse', 'Moselle', 'Yolande de Flandre', 'Henri de Bar', 'Robert de Cassel, apanagé', 'Flandre', 'Yolande, parti', 'comte de Flandre', 'Louis de Male', 'Henri IV']\n",
            "ENCPOS_1933_13\n",
            "\t ['Marseille', 'Ligures', 'Phocéens', 'Etrusques', 'Carthaginois', 'Gaulois', 'Espagne', 'Goths', 'Burgondes,', 'Visigoths', 'Ostrogoths', 'Sarrasins', 'Egypte', 'Byzance', 'Italie', 'Gênes', 'Notre-Dame de la Garde', 'Marseille Veyre', 'île de Riou', 'Grecs', 'Phéniciens', 'vallée du Rhône', 'Ligurie', 'comte de Provence']\n"
          ]
        }
      ],
      "source": [
        "# Save most representative entities of each entire doc\n",
        "# TODO ST redocumenter\n",
        "import itertools\n",
        "\n",
        "entities_by_doc={} #\n",
        "count=0\n",
        "from collections import Counter\n",
        "\n",
        "# voir avec ST : boucler sur doc_structured plutôt que docs_structured_sents\n",
        "# permet de réduire le nombre d'itération et de le réduire au nombre de docs chargés\n",
        "#for doc_id in docs_structured_sents.keys():\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  #print(doc_id)\n",
        "  macro_entidades=[]\n",
        "  for k,v in docs_structured_sents[doc_id].items():\n",
        "    if k!=\"metadata\":\n",
        "      for sent in v:\n",
        "        if len(sent)>10:\n",
        "          entidades=spacy_short_ner(str(sent), nlp)\n",
        "          # ?? sentence level\n",
        "          entidades=list(itertools.combinations(entidades, 2))\n",
        "          macro_entidades.extend(entidades)\n",
        "\n",
        "  nodes=[x[0] for x in Counter(list(sum(macro_entidades, ()))).most_common(25)]\n",
        "  nodes=[x for x in nodes if len(x)>4]\n",
        "  entities_by_doc[doc_id]=nodes\n",
        "  #print('\\t', entities_by_doc[doc_id])\n",
        "  count+=1\n",
        "  if count%500==0:\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7ELSeLe2c-e"
      },
      "outputs": [],
      "source": [
        "entities_by_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcSfdNpRocoC"
      },
      "outputs": [],
      "source": [
        "# TEST SKIP utile pour tester/redocumenter\n",
        "#extract contextual entites to build a Knownledge graph\n",
        "\n",
        "from collections import Counter\n",
        "macro_entidades=[]\n",
        "for k,v in docs_structured_sents[test_doc_id].items():\n",
        "  if k!=\"metadata\":\n",
        "    for sent in v:\n",
        "      if len(sent)>10:\n",
        "        entidades=spacy_short_ner(str(sent), nlp)\n",
        "        entidades=list(itertools.combinations(entidades, 2))\n",
        "        macro_entidades.extend(entidades)\n",
        "\n",
        "nodes=[x[0] for x in Counter(list(sum(macro_entidades, ()))).most_common(25)]\n",
        "nodes=[x for x in nodes if len(x)>4]\n",
        "nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HZHFSHRGshJ"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "#JSON dump of entities_by_doc (dict with chapters and sentences)\n",
        "import json\n",
        "with open('encpos_entities_by_doc.json', \"w\", encoding='utf8') as f:\n",
        "    json.dump(entities_by_doc, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQJfMTqxKx_i"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C0i2k9PKyXj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkKyVEq8Kyp3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc0ESOkaKzGb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYRPuxpaKzZR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQaqMDSmPFqk",
        "outputId": "1fce6262-f8ec-4e45-963e-615e5eff77cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Loading libraries and packages\n",
        "\n",
        "# nltk for stopwords and punct?\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('french')\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "#import wikipedia\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHnDy8X0KD7m"
      },
      "outputs": [],
      "source": [
        "#installing Flair, spacy, model languages and sentence transformers. This can take a while\n",
        "#remember delete the displayed information after installing\n",
        "#don't forget to switch to a GPU environment\n",
        "\n",
        "# Bert\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')#multilingual model for keywork extraction, text summarization and sentence transformation (0.7 PR)\n",
        "\n",
        "#!pip install Flair\n",
        "#!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4GJBsHj-0Xy"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Z7ikS6lex1"
      },
      "source": [
        "# Vectors dicts and Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNEVOmzTwFFY"
      },
      "outputs": [],
      "source": [
        "# Bert\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')#multilingual model for keywork extraction, text summarization and sentence transformation (0.7 PR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7fkgZMyHSM_"
      },
      "source": [
        "### Keywords vectors (`keywords_vectors`)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz1uBd1iE7uy"
      },
      "source": [
        "#### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAylhFv3K_KF",
        "outputId": "227fa701-207c-4297-94d6-30977f21f92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-20 11:58:58--  https://github.com/chartes/encpos_similarities/raw/master/models/encpos_keywords_vectors.npz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/chartes/encpos_similarities/master/models/encpos_keywords_vectors.npz [following]\n",
            "--2022-07-20 11:58:58--  https://raw.githubusercontent.com/chartes/encpos_similarities/master/models/encpos_keywords_vectors.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6852484 (6.5M) [application/octet-stream]\n",
            "Saving to: ‘encpos_keywords_vectors.npz.1’\n",
            "\n",
            "encpos_keywords_vec 100%[===================>]   6.53M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-07-20 11:58:58 (81.5 MB/s) - ‘encpos_keywords_vectors.npz.1’ saved [6852484/6852484]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/models/encpos_keywords_vectors.npz\n",
        "keywords_vectors_dump = np.load('encpos_keywords_vectors.npz')\n",
        "keywords_vectors = {doc_id:keywords_vectors_dump[doc_id] for doc_id in keywords_vectors_dump.files}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suMoCdDDS17v"
      },
      "outputs": [],
      "source": [
        "print(keywords_vectors[test_doc_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTSFBt49E8LR"
      },
      "source": [
        "#### Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM1d7qeNRAVJ"
      },
      "outputs": [],
      "source": [
        "# DEBUG VJ pour keywords\n",
        "# store keywords vector for each doc // LONG!\n",
        "\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "count=0\n",
        "keywords_vectors = {}  # dict pour stoker le vecteur des keywords pour chaque doc\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  keywords_vectors[doc_id]=model.encode([keywords_by_doc[doc_id]])\n",
        "\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-GTuIl-IIUo"
      },
      "outputs": [],
      "source": [
        "keywords_vectors[test_doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4cwPDSFKcqd"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "np.savez('encpos_keywords_vectors', **keywords_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZMh87aDz2Ki"
      },
      "outputs": [],
      "source": [
        "# utile où ???\n",
        "full_text_keywords = list(keywords_by_doc.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhGU9l0gHXrd"
      },
      "source": [
        "## Entities vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx3cEQage8l9"
      },
      "source": [
        "TODO : expliquer que cette représentation du texte ne sert finalement plus par la suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzrog442Ydeo"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG8CQgiwZGgx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/models/encpos_entities_vectors.npz\n",
        "entities_vectors_dump = np.load('encpos_entities_vectors.npz')\n",
        "entities_vectors = {doc_id:entities_vectors_dump[doc_id] for doc_id in entities_vectors_dump.files}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJdV9sh5D10P"
      },
      "outputs": [],
      "source": [
        "entities_vectors[test_doc_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0NIZRg2YfRv"
      },
      "source": [
        "### Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOdknBkcaOKj"
      },
      "outputs": [],
      "source": [
        "# DEBUG VJ pour entities – valider avec ST\n",
        "# store entities vector for each doc\n",
        "\n",
        "from collections import Counter\n",
        "entities_vectors={} # dict pour stcoker le vecteur des entités pour chaque doc\n",
        "\n",
        "count=0\n",
        "\n",
        "entities_dict={entity:i for i, entity in enumerate(Counter([y for x in entities_by_doc.values() for y in x]).keys())} # expliquer\n",
        "# print(entities_dict) # des occs à 0 – souhaité ?\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  ents=[entities_dict[x] for x in entities_by_doc[doc_id]] # mv entities entites_by_doc chargé depuis la source json (on ne calcule plus)\n",
        "  entities_vectors[doc_id]=ents\n",
        "\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)\n",
        "\n",
        "# entities_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9uWFV5JYmet"
      },
      "outputs": [],
      "source": [
        "entities_vectors[test_doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jot_BUOsY4Ug"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "import numpy as np\n",
        "np.savez('encpos_entities_vectors', **entities_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MAHIxLeHo-E"
      },
      "source": [
        "## Documents vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM_rSz4aeTNz"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgf9lCVPeY1M",
        "outputId": "7205f63c-f76f-4bdc-bc3a-997ec73f9456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-20 11:59:18--  https://github.com/chartes/encpos_similarities/raw/master/models/encpos_document_vectors.npz\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/chartes/encpos_similarities/master/models/encpos_document_vectors.npz [following]\n",
            "--2022-07-20 11:59:19--  https://raw.githubusercontent.com/chartes/encpos_similarities/master/models/encpos_document_vectors.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6852484 (6.5M) [application/octet-stream]\n",
            "Saving to: ‘encpos_document_vectors.npz’\n",
            "\n",
            "encpos_document_vec 100%[===================>]   6.53M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-07-20 11:59:19 (85.5 MB/s) - ‘encpos_document_vectors.npz’ saved [6852484/6852484]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/chartes/encpos_similarities/raw/master/models/encpos_document_vectors.npz\n",
        "document_vectors_dump = np.load('encpos_document_vectors.npz')\n",
        "document_vectors = {doc_id:document_vectors_dump[doc_id] for doc_id in document_vectors_dump.files}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqgD8rmder3H"
      },
      "outputs": [],
      "source": [
        "document_vectors[test_doc_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOKCCIPIeTqu"
      },
      "source": [
        "### Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ-_r4-37n6R"
      },
      "outputs": [],
      "source": [
        "# DEBUG VJ pour document_vectors – valider avec ST\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "count=0\n",
        "document_vectors={}\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  bloc = \"\"\n",
        "  for k,v in docs_structured[doc_id].items():\n",
        "    if k!=\"metadata\":\n",
        "      bloc+=v\n",
        "    # print(v)\n",
        "  doc_embedding = model.encode([bloc])\n",
        "  document_vectors[doc_id]=doc_embedding\n",
        "\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7H4U_SFA7V-"
      },
      "outputs": [],
      "source": [
        "document_vectors.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2yljk86dX_x"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "np.savez('encpos_document_vectors', **document_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOZwDpb_tfQ"
      },
      "source": [
        "## Doc2Vec (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0OxBAWC_4GG"
      },
      "source": [
        "### Download (`d2v_model`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "LkHl3kFhBM_9"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec\n",
        "d2v_model= Doc2Vec.load(\"https://github.com/chartes/encpos_similarities/raw/master/models/encpos_doc2vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YWfrshJ_-kZ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RLSQGyrAHQJ"
      },
      "outputs": [],
      "source": [
        "# Doc2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('french')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-j89JlXM2PP"
      },
      "outputs": [],
      "source": [
        "#transforming data\n",
        "data={k:\"\".join(list(v.values())).replace(\"=  \", \"\") for k,v in docs_structured.items()}\n",
        "data\n",
        "tagged_data = [TaggedDocument(words=[x for x in word_tokenize(v.lower()) if x not in stop_words], tags=[str(k)]) for k,v in data.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkRwEzFoWOxA"
      },
      "outputs": [],
      "source": [
        "# tagged_data[10][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6AhN7dmM2qi"
      },
      "outputs": [],
      "source": [
        "#modeling vectors, this can take a while\n",
        "max_epochs = 20\n",
        "vec_size = 30\n",
        "alpha = 0.025\n",
        "\n",
        "model_d2v = Doc2Vec(vector_size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1, window=10, workers=4)\n",
        "  \n",
        "model_d2v.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model_d2v.train(tagged_data,\n",
        "                total_examples=model_d2v.corpus_count,\n",
        "                epochs=model_d2v.iter)\n",
        "    # decrease the learning rate\n",
        "    model_d2v.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model_d2v.min_alpha = model_d2v.alpha\n",
        "\n",
        "print(\"end modelization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yTScAbXAsNF"
      },
      "outputs": [],
      "source": [
        "# Export du modèle\n",
        "model_d2v.save(\"encpos_doc2vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp_aUkHAH6Z-"
      },
      "source": [
        "## Memo Sergio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbRs_A0NM3zM"
      },
      "outputs": [],
      "source": [
        "#transforming each entities list and each text position into a vector\n",
        "#THIS IS A MANDATORY STEP AND CAN TAKE SEVERAL MINUTES\n",
        "\n",
        "from collections import Counter\n",
        "entities_vectors={}\n",
        "document_vectors={}\n",
        "keyword_vectors={}\n",
        "\n",
        "\n",
        "count=0\n",
        "\n",
        "entities_dict={entity:i for i, entity in enumerate(Counter([y for x in entities.values() for y in x]).keys())}\n",
        "\n",
        "full_text=[]\n",
        "full_text_keys=[]\n",
        "\n",
        "for item in entities.keys():\n",
        "  bloc=\"\"\n",
        "  keyword_bloc=\"\"\n",
        "  #ents=np.array([entities_dict[x] for x in entities[item]], ndmin=2)#2D array\n",
        "  ents=[entities_dict[x] for x in entities[item]]\n",
        "  for k,v in docs_structured[item].items(): # pourquoi pas sur docs_structured_sents ???\n",
        "    if k!=\"metadata\":\n",
        "      bloc+=v #join all chapters content without the title\n",
        "      \n",
        "      if len(v)>1:\n",
        "        #print(key_extractor(v))\n",
        "        try:\n",
        "          keyword_bloc+=\" \".join(key_extractor(v))+\" \"\n",
        "          \n",
        "        except:\n",
        "          continue\n",
        "  full_text.append(bloc) # on en fait quoi ? Pourquoi ?\n",
        "  full_text_keys.append(keyword_bloc)\n",
        "\n",
        "  doc_embedding = model.encode([bloc])\n",
        "  document_vectors[item]=doc_embedding\n",
        "  entities_vectors[item]=ents\n",
        "  keyword_vectors[item]=model.encode([keyword_bloc])\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)\n",
        "\n",
        "  #print(doc_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOEOIS_tNyvB"
      },
      "source": [
        "# Similarities (vector intersection ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDsIgMPh8XxI"
      },
      "outputs": [],
      "source": [
        "#cosine function to compute vectors similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cos_similarity(a,b):\n",
        "  \n",
        "  cos_sim=cosine_similarity(a, b, dense_output=False).tolist()[0][0]\n",
        "  return cos_sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e50REvvPWtyW"
      },
      "source": [
        "## Keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SIvaOSXv3JR",
        "outputId": "21da2c96-c466-416a-d720-334cdd23e942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENCPOS_2002_29\t0.9999999\t Dominique Stutzmann date: 2002 title: La bibliothèque de l’abbaye cistercienne de Fontenay (Côte-d’Or). Constitution, gestion, dissolution (xiie-xviiie siècle).    \n",
            "ENCPOS_1990_17\t0.84728587\t Françoise Simeray date: 1990 title: Le scriptorium et la bibliothèque de l’abbaye Saint-Amand    \n",
            "ENCPOS_2015_12\t0.8383621\t Hélène Jacquemard date: 2015 title: L’abbaye cistercienne de Vauclair et sa bibliothèque. Lire et écrire dans une abbaye cistercienne du Moyen Âge au xviiie siècle    \n",
            "ENCPOS_1969_09\t0.794045\t Marie-Pierre Laffitte-Pochat date: 1969 title: La bibliothèque et le scriptorium de Saint-Thierry de Reims    \n",
            "ENCPOS_1998_35\t0.79267997\t Laurent Veyssière date: 1998 title: Recueil des chartes de l’abbaye de Clairvaux.    \n",
            "ENCPOS_2009_25\t0.78906024\t Cécile Roger date: 2009 title: Guillaume de Saint-Lô, un prédicateur à l’œuvre au xive siècle    \n",
            "ENCPOS_2010_23\t0.7857414\t Cécile Roger date: 2010 title: Guillaume de Saint-Lô un prédicateur à l’œuvre au xive siècle    \n",
            "ENCPOS_2012_13\t0.774467\t Pauline Gendry date: 2012 title: Liturgie et vie monastique à l’abbaye bénédictine Saint-Martin de Savigny d’après son ordinaire médiéval. Édition critique et commentaire du ms. Archives départementales du Rhône, 1 H 20 (deuxième quart du xiiie siècle)    \n",
            "ENCPOS_1913_02\t0.7711915\t Eugène Berger date: 1913 title: Étude historique et archéologique sur l’abbaye de Saint-Père de Chartres    \n",
            "ENCPOS_1913_12\t0.7698721\t Joseph Macquart De Terline date: 1913 title: Étude sur l’abbaye de Cercamp (ordre de Cîteaux, diocèse d’Amiens)    \n"
          ]
        }
      ],
      "source": [
        "#similarity using KEYWORDS vectors\n",
        "search_term=keywords_vectors[test_doc_id]\n",
        "\n",
        "similar_terms=[[k, cosine_similarity(search_term, v)[0][0], docs_structured[k][\"metadata\"].split(\"creator:\")[1] ] for k,v in keywords_vectors.items() if len(docs_structured[k])>1]\n",
        "similar_terms=sorted(similar_terms, key = lambda x: x[1], reverse=True)[:10]\n",
        "for x in similar_terms:\n",
        "  print(*x, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHgNTE4SWx4B"
      },
      "source": [
        "## Entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaGwpn6Z5Fty",
        "outputId": "77ca920a-e4ff-4241-bc0e-90b92a56cc4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.14285714285714285, 'ENCPOS_1922_01', ['Paris', 'Delamare', 'Traité de la Police', 'roi Jean', 'Bureau de la ville', 'Édit du mois', 'Colbert']]\n",
            "[0.13953488372093023, 'ENCPOS_1986_15', ['Cisterciens', 'Paris', 'Clairvaux', 'Cîteaux', 'collège Saint-Bernard', 'Morimond', 'Benoît XII', 'Belgique', 'Occident', 'Étienne de Lexington', 'abbé de Clairvaux', 'Innocent IV', 'Jean Tolet', 'Alphonse de Poitiers', 'Montpellier', 'Estella', 'Toulouse', 'Oxford', 'Pontigny', 'Moyen Age', 'ordre de Cîteaux', 'Jean de Cirey']]\n",
            "[0.13636363636363635, 'ENCPOS_1968_07', ['Servien', 'Mazarin', 'Paris', 'Conseil', 'Richelieu', 'Cherasco', 'Condé', 'Affaires étrangères', 'Anjou', 'Longueville', 'union des Frondes', 'Poitiers', 'Baluze', 'Dupuy', 'Cinq-cents', 'Colbert', 'Bibliothèque nationale', 'Archives nationales', 'Abel Servien', 'Chavigny', 'Münster', 'l’Empire', 'comte d’Avaux']]\n",
            "[0.13636363636363635, 'ENCPOS_1970_11', ['Arras', 'Saint-Vaast', 'Jérémie', 'Cambrai', 'saint Vaast', 'Paris', 'Liber miraculorum', 'Arras 559', 'Robert de Molesme', 'Dijon', 'Douai', 'Albertus', 'Etienne Harding', 'Cîteaux', 'Oisbertus', 'Commentaire', 'saint Jérôme', 'Vitae Patrum', 'Bruxelles', 'Bibliothèque Royale de Belgique', '9850-9852', 'Grande Bible', 'Lamentations']]\n",
            "[0.13333333333333333, 'ENCPOS_1988_10', ['Baluze', 'Limousin', 'Tulle', 'Paris', 'Uzerche', 'Vigeois', 'Colbert', 'Maison d’Auvergne', 'Limoges', 'Adhémar de Chabannes', 'St-Martin', 'abbayes de Dalon', 'Turenne', 'Pompadour', 'Toulouse', 'Armand Gérard', 'Mabillon', 'Hérouval', 'Gaignières', 'Bas-Limousin', 'Obazine, Beaulieu', 'Meymac', 'Moyen Age', 'Miscellanea']]\n",
            "[0.13333333333333333, 'ENCPOS_1965_11', ['Archives nationales', 'Bibliothèque du roi', 'Paris', 'Mabillon', 'Baluze', 'Gaignières', 'abbé Lebeuf', 'Delisle', 'Luchaire', 'Lasteyrie', 'abbé Alliot', 'Archives de Seine-et-Oise', 'Étienne de Senlis', 'Antoine Véronneau', 'Yerres', 'K 179', 'Q115071-3', 'roi de France', 'Louis VI', 'Eustachie de Corbeil', 'Saint-Jacques', 'Etienne de Senlis', 'Cîteaux', 'Saint-Nicolas']]\n",
            "[0.13333333333333333, 'ENCPOS_1977_03', ['Tractatus', 'Étienne de Bourbon', 'Humbert de Romans', 'saint Augustin', 'Paris', 'Pères de l’Église', 'saint Grégoire', 'Moralia in Job', 'Homeliae in Evangelia', 'Enarrationes in Psalmos', 'Pseudo-Augustin', 'De vera', 'falsa poenitentia', 'saint Jérôme', 'saint Ambroise', 'saint Isidore de Séville', 'Dombes', 'Beaujolais', 'Forez', 'Massif central', 'Bourgogne', 'Champagne', 'Savoie', 'Piémont']]\n",
            "[0.13333333333333333, 'ENCPOS_2001_11', ['Guy Patin', 'Paris', 'Parlement', 'Baluze', 'Richelieu', 'Mazarin', 'la France', 'guerre de Trente Ans', 'rébellion de Gaston d’Orléans', 'révolte du', 'Languedoc', 'Louis XIII', 'Colbert', 'Faculté de Paris', 'William Harvey', 'Jean Pecquet', 'France', 'Patin', 'Gaston d’Orléans', 'Faculté de médecine de Paris', 'Charles Spon', 'Anne d’Autriche', 'Théophraste Renaudot', 'Faculté de Montpellier']]\n",
            "[0.13333333333333333, 'ENCPOS_1901_02', ['La Reynie', 'Limoges', 'Bordeaux', 'Jean Nicolas', 'Tralage', 'Jean Nicolas de Tralage', 'Gabriel Nicolas de La Reynie', 'Françoise Nicolas', 'Françoise de Sainte-Thérèse', '– Gabriel', 'Châtelet', 'Paris', 'Colbert', 'Mignard', 'Louvois', 'Parlement', 'Guy Patin', 'Baluze', 'Saint-Simon', 'Chapelain', 'Seignelay', 'Coudray', 'Sagot', 'Gaudion']]\n",
            "[0.13043478260869565, 'ENCPOS_1953_16', ['Beleth', 'Summa', 'Moyen Age', 'Lauriman', 'de France', 'Italie', 'Empire', 'Angleterre', 'Rationale', 'Somme', 'Guillaume Durand', 'P. Van den Eyde', 'Élizabeth de Schönau', 'Jean Beleth', 'Pères latins', 'saint Bernard', 'Hugues', 'Richard de Saint-Victor', 'Paris', 'Éclaircissements', 'l’Écriture', 'Cîteaux', 'Le Long', 'Amalaire', 'Durand']]\n"
          ]
        }
      ],
      "source": [
        "# Similarity using ENTITIES vectors\n",
        "cherche=entities_by_doc[test_doc_id]\n",
        "similar_docs_number = 10\n",
        "similar_docs = []\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  try:\n",
        "    coeff=(2*len(list(set(cherche) & set(entities_by_doc[doc_id]))))/(len(cherche)+len(entities_by_doc[doc_id]))\n",
        "    if test_doc_id == doc_id:\n",
        "      continue\n",
        "    if coeff>0.05:\n",
        "      similar_docs.append([coeff, doc_id, entities_by_doc[doc_id]])\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "similar_docs.sort(key=lambda x:x[0], reverse=True)\n",
        "#similar_docs[0:similar_docs_number]\n",
        "for l in similar_docs[0:similar_docs_number]:\n",
        "  print(l[0:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcxWALkAW3En"
      },
      "source": [
        "## Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_dsvkYGhX2y",
        "outputId": "e6bdc37e-057a-4c9a-e21f-220417ef7e30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENCPOS_2002_29\t0.9999999\t Dominique Stutzmann date: 2002 title: La bibliothèque de l’abbaye cistercienne de Fontenay (Côte-d’Or). Constitution, gestion, dissolution (xiie-xviiie siècle).    \n",
            "ENCPOS_2016_21\t0.73231614\t Clémentine Villien date: 2016 title: L'église abbatiale cistercienne Notre-Dame d’Acey. Étude historique, architecturale et archéologique    \n",
            "ENCPOS_2015_12\t0.70256317\t Hélène Jacquemard date: 2015 title: L’abbaye cistercienne de Vauclair et sa bibliothèque. Lire et écrire dans une abbaye cistercienne du Moyen Âge au xviiie siècle    \n",
            "ENCPOS_1926_02\t0.68688256\t Anne-Marie Aubert date: 1926 title: Histoire et développement économique d’une abbaye cistercienne, Bellevaux en Franche-Comté (xiie-xvie siècle)    \n",
            "ENCPOS_1951_01\t0.68668956\t Bernard Bagneris date: 1951 title: La vie économique de l’abbaye de Montier-la-Celle du xive au xviie siècle    \n",
            "ENCPOS_1997_08\t0.68384457\t Stéphanie Billot date: 1997 title: Trois-Fontaine, fille aînée de Clairvaux : étude et édition du chartrier (1118-1231)    \n",
            "ENCPOS_1998_35\t0.66649675\t Laurent Veyssière date: 1998 title: Recueil des chartes de l’abbaye de Clairvaux.    \n",
            "ENCPOS_1994_02\t0.65643954\t Aline Berstein date: 1994 title: L’abbaye de Chaalis. Édition et présentation du chartrier (1136-1204)    \n",
            "ENCPOS_1937_12\t0.6542878\t Marie-Marguerite Lemarignier date: 1937 title: Les cartulaires de Saint-Mesmin de Micy. Étude critique et essai de restitution    \n",
            "ENCPOS_1934_09\t0.6535791\t Françoise Jean-Lehoux date: 1934 title: Le bourg Saint-Germain-des-Prés des origines au xve siècle    \n"
          ]
        }
      ],
      "source": [
        "#similarity using DOCUMENT vectors\n",
        "search_term=document_vectors[test_doc_id]\n",
        "\n",
        "'''\n",
        "for k,v in document_vectors.items():\n",
        "  cos=cosine_similarity(search_term, v)\n",
        "  if cos>0.45:\n",
        "    print(k, \"\\t\", cos[0], \"\\t\", docs_structured[k][\"metadata\"].split(\"creator:\")[1])\n",
        "'''\n",
        "\n",
        "similar_terms=[[k, cosine_similarity(search_term, v)[0][0], docs_structured[k][\"metadata\"].split(\"creator:\")[1] ] for k,v in document_vectors.items() if len(docs_structured[k])>1]\n",
        "similar_terms=sorted(similar_terms, key = lambda x: x[1], reverse=True)[:10]\n",
        "for x in similar_terms:\n",
        "  print(*x, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZqLzbzeW8x2"
      },
      "source": [
        "## Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd43NHlEQDi_",
        "outputId": "99f50930-24be-4780-805f-b4fd977acc91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENCPOS_1933_13 0.44863  Régine Pernoud date: 1933 title: Essai sur le port de Marseille des origines à la fin du xiiie siècle    \n",
            "ENCPOS_1994_04 0.40778  Michelle Bubenicek date: 1994 title: Le pouvoir au féminin. Une princesse en politique et son entourage : Yolande de Flandre, comtesse de Bar et dame de Cassel (1326-1395)    \n",
            "ENCPOS_1971_12 0.37944  Alain Guerreau date: 1971 title: Une ville et ses finances : Mâcon    \n",
            "ENCPOS_1971_25 0.37358  Jean-Claude Schmitt date: 1971 title: L’Église et les clercs face aux béguines et aux beghards du Rhin supérieur du xive siècle au xve siècle    \n",
            "ENCPOS_1971_15 0.34851  Danielle Jacquart date: 1971 title: Un médecin parisien du xve siècle, Jacques Despars (1380-1458)    \n",
            "ENCPOS_2003_19 0.32172  Sébastien Nadiras date: 2003 title: Guillaume de Nogaret et la pratique du pouvoir    \n",
            "ENCPOS_1972_18 0.31963  Michel Pastoureau date: 1972 title: Le bestiaire héraldique au Moyen Âge    \n",
            "ENCPOS_1999_35 0.30514  Elsa Marguin date: 1999 title: L’Ars lectoria Ecclesie de Jean de Garlande. Étude, édition et traduction    \n",
            "ENCPOS_2000_08 0.28998  Olivier Canteaut date: 2000 title: Philippe V et son Conseil : le gouvernement royal de 1316 à 1322    \n"
          ]
        }
      ],
      "source": [
        "#similarity using Doc2Vec model\n",
        "similar_doc = d2v_model.docvecs.most_similar(test_doc_id, topn=40)\n",
        "for x in similar_doc:\n",
        "  try:\n",
        "    print(x[0], round(x[1], 5), docs_structured[x[0]][\"metadata\"].split(\"creator:\")[1])\n",
        "  except:\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LnjpCvSOfW8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-kT7in4Ofuk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_2zBfMeOgH1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjYV_ObXAto8"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keywords similarities matrix"
      ],
      "metadata": {
        "id": "ooXwzKfew2TF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bC63umH0V5k",
        "outputId": "aa913550-0a9a-4a57-d0c0-01772293a169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n"
          ]
        }
      ],
      "source": [
        "# Calculate keywords similarities matrix 3000 X 3000\n",
        "\n",
        "matrix={}\n",
        "liste_encpos=list(docs_structured.keys())\n",
        "\n",
        "for i, pos in enumerate(liste_encpos):\n",
        "  matrix[pos]=[]\n",
        "  for pos_b in liste_encpos:\n",
        "    if pos!=pos_b:\n",
        "          matrix[pos].append([pos_b, cos_similarity(keywords_vectors[pos], keywords_vectors[pos_b])])\n",
        "  if i%500==0:\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export keywords_similarities_matrix\n",
        "import json\n",
        "with open('keywords_similarities_matrix.json', 'w', encoding='utf8') as f:\n",
        "    json.dump(matrix, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "hh7d7CxwrZB_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document similarities matrix"
      ],
      "metadata": {
        "id": "LyteXy1sw8dh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJawL141zKJB",
        "outputId": "11c143a0-6d7b-487b-8814-d50febba02dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n"
          ]
        }
      ],
      "source": [
        "# Calculate document similarities matrix 3000x3000\n",
        "\n",
        "matrix_docs={}\n",
        "liste_encpos=list(docs_structured.keys())\n",
        "\n",
        "for i, pos in enumerate(liste_encpos):\n",
        "  matrix_docs[pos]=[]\n",
        "  for pos_b in liste_encpos:\n",
        "    if pos!=pos_b:\n",
        "          matrix_docs[pos].append([pos_b, cos_similarity(document_vectors[pos], document_vectors[pos_b])])\n",
        "  if i%500==0:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JvZPIGGYzOaY"
      },
      "outputs": [],
      "source": [
        "# Export document_similarities_matrix\n",
        "import json\n",
        "with open('document_similarities_matrix.json', 'w', encoding='utf8') as f:\n",
        "    json.dump(matrix, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec similarities matrix"
      ],
      "metadata": {
        "id": "zNygVoGNzRly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sir56KfD8SM"
      },
      "outputs": [],
      "source": [
        "# Calculate Doc2Vec similarities matrix 3000x3000\n",
        "\n",
        "matrix_d2v={}\n",
        "liste_encpos=list(docs_structured.keys())\n",
        "\n",
        "for i, pos in enumerate(liste_encpos):\n",
        "  for pos_b in liste_encpos:\n",
        "    if pos!=pos_b:\n",
        "      coeff=cos_similarity(model_d2v.docvecs[pos].reshape(1, -1), model_d2v.docvecs[pos_b].reshape(1, -1))\n",
        "      for ii, x in enumerate(matrix_d2v[pos]):\n",
        "        if pos_b in x[0]:\n",
        "          matrix_d2v[pos][ii]=matrix_d2v[pos][ii]+[coeff]\n",
        "  if i%500==0:\n",
        "    print(i)\n",
        "\n",
        "import json\n",
        "with open('doc2vec_similarities_matrix.json', 'w') as fp:\n",
        "    json.dump(matrix_d2v, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN8D74wwHb9x"
      },
      "outputs": [],
      "source": [
        "cos_similarity(model_d2v.docvecs[\"ENCPOS_2000_10\"].reshape(1, -1), model_d2v.docvecs[\"ENCPOS_2000_09\"].reshape(1, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mesure de la déviation entre les scores de similarité\n",
        "\n",
        "TODO: expliquer"
      ],
      "metadata": {
        "id": "YvMgk4Vx31Fu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcRHnL8lzVy4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#full_text=[]\n",
        "#full_text_keys=[]\n",
        "data=[len(x.split()) for x in full_text_keys]\n",
        "\n",
        "from statistics import mean, median,variance,stdev\n",
        "\n",
        "m = mean(data)\n",
        "median = median(data)\n",
        "variance = variance(data)\n",
        "stdev = stdev(data)\n",
        "print('average: {0:.2f}'.format(m))\n",
        "print('Median: {0:.2f}'.format(median))\n",
        "print('Distributed: {0:.2f}'.format(variance))\n",
        "print('standard deviation: {0:.2f}'.format(stdev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihgfwy0cLdY8"
      },
      "source": [
        "# A insérer ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhgvryOUAIfW"
      },
      "source": [
        "### entity linking wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV16nX-oZx4T"
      },
      "outputs": [],
      "source": [
        "# SKIP\n",
        "#entity linking by using french and english wikipedia \n",
        "for node in nodes:\n",
        "  try:\n",
        "    wikipedia.set_lang(\"fr\")\n",
        "    x=wikipedia.page(node, auto_suggest=False)\n",
        "    print(node, x.url)\n",
        "  except (wikipedia.exceptions.DisambiguationError) as e:\n",
        "    try:\n",
        "      options=e.options\n",
        "      #x=[y for y in x if any(z in wikipedia.summary(y, auto_suggest=False) for z in nodes)]\n",
        "      print(options)\n",
        "    except:\n",
        "      continue\n",
        "  except wikipedia.PageError:\n",
        "    wikipedia.set_lang(\"en\")\n",
        "    try:\n",
        "      x=wikipedia.page(node, auto_suggest=False)\n",
        "      print(node, x.url)\n",
        "    except (wikipedia.exceptions.DisambiguationError, wikipedia.PageError) as e:\n",
        "      try:\n",
        "        options=e.options\n",
        "        #x=[y for y in x if any(z in wikipedia.summary(y, auto_suggest=False) for z in nodes)]\n",
        "        print(options)\n",
        "      except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdUaJyaNwQsc"
      },
      "outputs": [],
      "source": [
        "cherche=model.encode([\"Auxerrois\"])\n",
        "\n",
        "for k,v in document_vectors.items():\n",
        "  cos=cosine_similarity(cherche, v)\n",
        "  if cos>0.1:\n",
        "    print(k, \"\\t\", cos[0], \"\\t\", docs_structured[k][\"metadata\"].split(\"creator:\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aietC46o7AzT"
      },
      "outputs": [],
      "source": [
        "cherche=entities_by_doc[\"ENCPOS_1972_18\"]\n",
        "\n",
        "for item in entities_by_doc.keys():\n",
        "  try:\n",
        "    a=[x for x in cherche if x in entities_by_doc[item]]\n",
        "    #list(set(cherche) & set(entities[item]))\n",
        "    if len(a)>3:\n",
        "      print(item, docs_structured[item][\"metadata\"].split(\"creator:\")[1], a)\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUQv6zy37njk",
        "outputId": "375d2245-0056-4640-96e8-d9ad6fb4090f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14194782078266144"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "cos_similarity(document_vectors[\"ENCPOS_2011_08\"], document_vectors[\"ENCPOS_1988_10\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "encpos_similarities.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}