{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "vOsurVv1t1wu"
      },
      "outputs": [],
      "source": [
        "test_doc_id = 'ENCPOS_2002_29' # ENCPOS_1972_18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3XT8vELcCBd"
      },
      "source": [
        "# Loading the dataset (`docs_structured`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BWmUsOQKd1T"
      },
      "outputs": [],
      "source": [
        "#download, unzip and reading encpos dataset\n",
        "import glob\n",
        "\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/data/encpos_txt.zip\n",
        "!unzip encpos_txt.zip -d /content/\n",
        "\n",
        "docs = [] # list of documents (a single line string for each doc)\n",
        "docs=[open(filename, \"r\").readlines() for filename in glob.glob(\"/content/encpos_txt/*.txt\")]\n",
        "docs=[\" \".join(x).replace(\"\\n\", \"\") for x in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HlcdckuHmArs"
      },
      "outputs": [],
      "source": [
        "# Extracting chapters structuration from texts\n",
        "\n",
        "import re\n",
        "\n",
        "docs_structured={} # dict of docs to process ('metadata' + one item for each text div)\n",
        "for index, doc in enumerate(docs):\n",
        "  chapters=re.findall(r\"==.+?\\==\" , doc) #match chapters (and subchaters) titles as the pattern is \"===\" and \"==\"\n",
        "  regexPattern = '|'.join(map(re.escape, chapters))\n",
        "  a=re.split(regexPattern, doc)\n",
        "\n",
        "  text_structured={x:y for x,y in list(zip([\"metadata\"]+chapters, a))}\n",
        "  try:\n",
        "    identifier=text_structured[\"metadata\"].split(\"identifier: \")[1].split(\" \", 1)[0].replace(\" \", \"\")\n",
        "    docs_structured[identifier]=text_structured\n",
        "  except:\n",
        "    #print(text_structured[\"metadata\"])\n",
        "    identifier=doc.split(\"identifier: \")[1].split(\" \",1)[0]\n",
        "    resume=doc.split(\"title: \")[1]\n",
        "    docs_structured[identifier]={\"title\":resume}\n",
        "    #print(texts[index], \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39TpbPeI_OHe"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "docs_structured[test_doc_id]['metadata']\n",
        "docs_structured[test_doc_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqZJyN7NFKSE"
      },
      "source": [
        "# Features extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYNyne3cJQnw"
      },
      "source": [
        "## Sentences segmentation (`docs_structured_sents`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ha93vK3qfP"
      },
      "source": [
        "#### 1. Télécharger le corpus segmenté en phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTS-GMpf194I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_structured_sents.json\n",
        "\n",
        "docs_structured_sents = {}  # dict of docs (with their 'metadata' + one item for each text div with a list of its sentences)\n",
        "\n",
        "with open('/content/encpos_structured_sents.json') as json_file:\n",
        "  docs_structured_sents = json.load(json_file)\n",
        "\n",
        "# docs_structured_sents.keys()\n",
        "# docs_structured_sents['ENCPOS_2002_29']['metadata']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYcMl2kVAEZK"
      },
      "source": [
        "#### 2. (OU) Segmenter le corpus en phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_C-oGPOrQmd"
      },
      "outputs": [],
      "source": [
        "#Option 2: extracting chapter structuration with sentences split (it can take a while)\n",
        "\n",
        "# Spacy required for senteces segmentation\n",
        "!pip install -U spacy[cuda92,transformers,lookups] # tout est requis ici ?\n",
        "!python -m spacy download fr_core_news_lg\n",
        "import spacy\n",
        "nlp=spacy.load(\"fr_core_news_lg\") #spacy linguistic model for french news large\n",
        "\n",
        "docs_structured_sents={} # dict of docs ('metadata' + one item for each text div with a list of its sentences)\n",
        "for index, doc in enumerate(docs):\n",
        "  chapters=re.findall(r\"==.+?\\==\" , doc) #chapters and subchaters as the pattern is \"===\" and \"==\"\n",
        "  regexPattern = '|'.join(map(re.escape, chapters))\n",
        "  a=re.split(regexPattern, doc)\n",
        "\n",
        "  text_structured={x:y for x,y in list(zip([\"metadata\"]+chapters, a))}\n",
        "  try:\n",
        "    identifier=text_structured[\"metadata\"].split(\"identifier: \")[1].split(\" \", 1)[0].replace(\" \", \"\")\n",
        "    text_structured={k:[sent.text for sent in nlp(v).sents] for k,v in text_structured.items()}\n",
        "    docs_structured_sents[identifier]=text_structured\n",
        "  except:\n",
        "    #print(text_structured[\"metadata\"])\n",
        "    identifier=doc.split(\"identifier: \")[1].split(\" \",1)[0]\n",
        "    #resume=doc.split(\"title: \")[1]\n",
        "    docs_structured_sents[identifier]={\"metadata\":[sent.text for sent in nlp(doc).sents]}\n",
        "  if index%400==0: #there are >2900 positions\n",
        "    print(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg9IWPMQ6eWr"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "docs_structured_sents[test_doc_id]['metadata']\n",
        "docs_structured_sents[test_doc_id]['== Conclusion ==']\n",
        "# docs_structured_sents[test_doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFiBD61IwRL5"
      },
      "outputs": [],
      "source": [
        "# Export `docs_structured_sents`\n",
        "\n",
        "#JSON or pickle dump of docs_structured_sents (dict with chapters and sentences)\n",
        "import json\n",
        "with open('encpos_structured_sents.json', \"w\", encoding='utf8') as f:\n",
        "    json.dump(docs_structured_sents, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwgN-VTeEbbN"
      },
      "source": [
        "## Keywords extraction (`keywords_by_doc`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Z_vW3JEfBR"
      },
      "source": [
        "### Télécharger la liste des keywords par doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9vMbKEHRLAj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_keywords_by_doc.json\n",
        "keywords_by_doc = {}        # dict of docs with their discriminant keywords\n",
        "with open('/content/encpos_keywords_by_doc.json') as json_file:\n",
        "  keywords_by_doc = json.load(json_file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrzZduqy1ETX"
      },
      "outputs": [],
      "source": [
        "keywords_by_doc[test_doc_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXi3zyOjEl9Y"
      },
      "source": [
        "### (OU) Calculer les keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PTKvvSjZQuQ"
      },
      "outputs": [],
      "source": [
        "#Loading extraction functions\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Bert\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased-v1') #multilingual model for keywork extraction, text summarization and sentence transformation (0.7 PR)\n",
        "\n",
        "# nltk for stopwords and punct? Only HERE ?\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('french')\n",
        "\n",
        "!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# test, régler fuzz\n",
        "# fuzz.ratio('bibliothécaires', 'bibliothèques')\n",
        "\n",
        "\"\"\"\n",
        "The idea behind is to compare the full paragraph embbeding against all possible 8-words combination embeddings (candidates)\n",
        "Then, we select the 8-words closest (by cosine similarity) to the full paragraph embeddings, as they are our more representative keywords.  \n",
        "\"\"\"\n",
        "def key_extractor(doc, top_n=8, n_gram_range = (1, 1)): #function to extract keywords from a text, n_gram_range indicates the matrice range of candidates\n",
        "  vectorizer = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
        "\n",
        "  candidates = vectorizer.get_feature_names_out() \n",
        "  candidate_embeddings = model.encode(candidates) # fréquence des mots plus important que leur ordre (embedding de la fréquence du lexique dans chaque bloc) ? \n",
        "\n",
        "  doc_embedding = model.encode([doc])\n",
        "\n",
        "  distances = cosine_similarity(doc_embedding, candidate_embeddings) # expliquer\n",
        "  keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "\n",
        "  #optional to filter similar keywords inside the same group (vg. Bibliothèque, bibliothècaire)\n",
        "  keywords_fuzzy=[]\n",
        "  for i, x in enumerate(keywords):\n",
        "    if i>0:\n",
        "      if any(fuzz.ratio(y,x)>85 for y in keywords_fuzzy):\n",
        "        continue\n",
        "      else:\n",
        "        keywords_fuzzy.append(x)\n",
        "    else:\n",
        "      keywords_fuzzy.append(x)\n",
        "\n",
        "  return keywords_fuzzy\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TODO: filtrer les keywords trop proches – cf fuzzy method de key_extractor(), en mieux\n",
        "\"\"\"\n",
        "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):#extraction by cosine embeddings similarity\n",
        "    # Calculate distances and extract keywords\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
        "                                            candidate_embeddings)\n",
        "\n",
        "    # Get top_n words as candidates based on cosine similarity\n",
        "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "    words_vals = [candidates[index] for index in words_idx]\n",
        "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "    # Calculate the combination of words that are the least similar to each other\n",
        "    min_sim = np.inf\n",
        "    candidate = None\n",
        "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "        if sim < min_sim:\n",
        "            candidate = combination\n",
        "            min_sim = sim\n",
        "\n",
        "    return [words_vals[idx] for idx in candidate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KKiawI2TThl"
      },
      "outputs": [],
      "source": [
        "# DEBUG VJ pour keywords\n",
        "# store keywords vector for each doc\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "count=0\n",
        "keywords_by_doc = {} # stocker les mots clé de chaque doc\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  # print(doc_id)\n",
        "  keyword_bloc=\"\"\n",
        "  for k,v in docs_structured[doc_id].items():\n",
        "    if k!=\"metadata\":\n",
        "      if len(v)>10: # des chapitres avec uniquement des sauts de lignes…\n",
        "        try:\n",
        "          keyword_bloc+=\" \".join(key_extractor(v))+\" \"\n",
        "        except:\n",
        "          continue\n",
        "  keywords_by_doc[doc_id] = keyword_bloc\n",
        "\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wh8YDGvPwB-"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "#JSON dump of keywords_by_doc (dict with chapters and sentences)\n",
        "import json\n",
        "with open('encpos_keywords_by_doc.json', \"w\", encoding='utf8') as f:\n",
        "    json.dump(keywords_by_doc, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZXIGkOUaV7A"
      },
      "source": [
        "## Entities extraction (`entities_by_doc`)\n",
        "\n",
        "TODO. Expliquer pourquoi on travaille au niveau de la phrase (`docs_structured_sents`)\n",
        "\n",
        "Long à calculer. On peut :\n",
        "\n",
        "1. Télécharger la liste déjà calculée\n",
        "1. Calculer la *liste*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idON8EX-Dl-y"
      },
      "source": [
        "#### 1. Télécharger la liste des entités par doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuAHzn5tDRZP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/data_structured/encpos_entities_by_doc.json\n",
        "entities_by_doc = {}        # dict of docs with their discriminant entities\n",
        "with open('/content/encpos_entities_by_doc.json') as json_file:\n",
        "  entities_by_doc = json.load(json_file) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftClGbX91x6x"
      },
      "outputs": [],
      "source": [
        "entities_by_doc['test_doc_id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Crz0LED3-N"
      },
      "source": [
        "#### 2. (OU) Calculer les entités discriminantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9IXVNufjkuF"
      },
      "outputs": [],
      "source": [
        "#installing spacy\n",
        "!pip install -U pip setuptools wheel # ?\n",
        "!pip install -U spacy[cuda92,transformers,lookups]\n",
        "import spacy\n",
        "\n",
        "#Download language model for modern french. If you get a downloading error you must restart the runtime\n",
        "!python -m spacy download fr_core_news_lg\n",
        "nlp=spacy.load(\"fr_core_news_lg\") #spacy linguistic model for french news large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za5ymAPA4VAn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "function to extract meaning relationships between entities based on (Spacy) dependencies\n",
        "TODO\n",
        "text = doc ? ou sentence ?\n",
        "\"\"\"\n",
        "def rel_extraction(text):\n",
        "  keys=[\"ROOT\", \"nsubj\"]\n",
        "  keys_2=[\"LOC\", \"PER\", \"ORG\"]\n",
        "\n",
        "  doc_dep = nlp(text)\n",
        "  doc_dep=[[tok.text, tok.dep_] for tok in doc_dep]\n",
        "\n",
        "  doc_ents= spacy_large_ner(text, nlp)\n",
        "  doct_ents=list(doc_ents)\n",
        "  doc_rel=[\"O\"]*len(doc_dep)\n",
        "  for ent in list(doc_ents): doc_rel[ent[-2]:ent[-1]]=[ent[-3]]*(ent[-1]-ent[-2])\n",
        "\n",
        "  #doc_merged=[\"\\t\".join(x+[y]) for x,y in list(zip(doc_dep, doc_rel))]\n",
        "  doc_merged=[x+[y] for x,y in list(zip(doc_dep, doc_rel))]\n",
        "\n",
        "  doc_merged=[x[0] for x in doc_merged if x[1] in keys or x[2] in keys_2 ]\n",
        "\n",
        "  return doc_merged\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "extract entities and character indexes\n",
        "TODO\n",
        "\"\"\"\n",
        "def spacy_large_ner(document, model):\n",
        "  return {(ent.text.strip(), ent.label_, ent.start, ent.end) for ent in model(document).ents}\n",
        "\n",
        "\"\"\"\n",
        "extract entities\n",
        "TODO\n",
        "\"\"\"\n",
        "def spacy_short_ner(document, model): #extract just entities text\n",
        "  return [ent.text.strip() for ent in model(document).ents]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41Q-G_uEWZ71"
      },
      "outputs": [],
      "source": [
        "# Save most representative entities of each entire doc\n",
        "# TODO ST redocumenter\n",
        "import itertools\n",
        "\n",
        "entities_by_doc={} #\n",
        "count=0\n",
        "from collections import Counter\n",
        "\n",
        "# voir avec ST : boucler sur doc_structured plutôt que docs_structured_sents\n",
        "# permet de réduire le nombre d'itération et de le réduire au nombre de docs chargés\n",
        "#for doc_id in docs_structured_sents.keys():\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  #print(doc_id)\n",
        "  macro_entidades=[]\n",
        "  for k,v in docs_structured_sents[doc_id].items():\n",
        "    if k!=\"metadata\":\n",
        "      for sent in v:\n",
        "        if len(sent)>10:\n",
        "          entidades=spacy_short_ner(str(sent), nlp)\n",
        "          # ?? sentence level\n",
        "          entidades=list(itertools.combinations(entidades, 2))\n",
        "          macro_entidades.extend(entidades)\n",
        "\n",
        "  nodes=[x[0] for x in Counter(list(sum(macro_entidades, ()))).most_common(25)]\n",
        "  nodes=[x for x in nodes if len(x)>4]\n",
        "  entities_by_doc[doc_id]=nodes\n",
        "  #print('\\t', entities_by_doc[doc_id])\n",
        "  count+=1\n",
        "  if count%500==0:\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcSfdNpRocoC"
      },
      "outputs": [],
      "source": [
        "# TEST SKIP utile pour tester/redocumenter\n",
        "#extract contextual entites to build a Knownledge graph\n",
        "\n",
        "from collections import Counter\n",
        "macro_entidades=[]\n",
        "for k,v in docs_structured_sents[test_doc_id].items():\n",
        "  if k!=\"metadata\":\n",
        "    for sent in v:\n",
        "      if len(sent)>10:\n",
        "        entidades=spacy_short_ner(str(sent), nlp)\n",
        "        entidades=list(itertools.combinations(entidades, 2))\n",
        "        macro_entidades.extend(entidades)\n",
        "\n",
        "nodes=[x[0] for x in Counter(list(sum(macro_entidades, ()))).most_common(25)]\n",
        "nodes=[x for x in nodes if len(x)>4]\n",
        "nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HZHFSHRGshJ"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "#JSON dump of entities_by_doc (dict with chapters and sentences)\n",
        "import json\n",
        "with open('encpos_entities_by_doc.json', \"w\", encoding='utf8') as f:\n",
        "    json.dump(entities_by_doc, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYRPuxpaKzZR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQaqMDSmPFqk"
      },
      "outputs": [],
      "source": [
        "#Loading libraries and packages\n",
        "\n",
        "# nltk for stopwords and punct?\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('french')\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "#import wikipedia\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHnDy8X0KD7m"
      },
      "outputs": [],
      "source": [
        "#installing Flair, spacy, model languages and sentence transformers. This can take a while\n",
        "#remember delete the displayed information after installing\n",
        "#don't forget to switch to a GPU environment\n",
        "\n",
        "# Bert\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')#multilingual model for keywork extraction, text summarization and sentence transformation (0.7 PR)\n",
        "\n",
        "#!pip install Flair\n",
        "#!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4GJBsHj-0Xy"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Z7ikS6lex1"
      },
      "source": [
        "# Vectors dicts and Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNEVOmzTwFFY"
      },
      "outputs": [],
      "source": [
        "# Bert\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')#multilingual model for keywork extraction, text summarization and sentence transformation (0.7 PR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7fkgZMyHSM_"
      },
      "source": [
        "### Keywords vectors (`keywords_vectors`)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz1uBd1iE7uy"
      },
      "source": [
        "#### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAylhFv3K_KF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/models/encpos_keywords_vectors.npz\n",
        "keywords_vectors_dump = np.load('encpos_keywords_vectors.npz')\n",
        "keywords_vectors = {doc_id:keywords_vectors_dump[doc_id] for doc_id in keywords_vectors_dump.files}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suMoCdDDS17v"
      },
      "outputs": [],
      "source": [
        "print(keywords_vectors[test_doc_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTSFBt49E8LR"
      },
      "source": [
        "#### Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM1d7qeNRAVJ"
      },
      "outputs": [],
      "source": [
        "# DEBUG VJ pour keywords\n",
        "# store keywords vector for each doc // LONG!\n",
        "\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "count=0\n",
        "keywords_vectors = {}  # dict pour stoker le vecteur des keywords pour chaque doc\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  keywords_vectors[doc_id]=model.encode([keywords_by_doc[doc_id]])\n",
        "\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-GTuIl-IIUo"
      },
      "outputs": [],
      "source": [
        "keywords_vectors[test_doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4cwPDSFKcqd"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "np.savez('encpos_keywords_vectors', **keywords_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZMh87aDz2Ki"
      },
      "outputs": [],
      "source": [
        "# utile où ???\n",
        "full_text_keywords = list(keywords_by_doc.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhGU9l0gHXrd"
      },
      "source": [
        "## Entities vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx3cEQage8l9"
      },
      "source": [
        "TODO : expliquer que cette représentation du texte ne sert finalement plus par la suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzrog442Ydeo"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG8CQgiwZGgx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/models/encpos_entities_vectors.npz\n",
        "entities_vectors_dump = np.load('encpos_entities_vectors.npz')\n",
        "entities_vectors = {doc_id:entities_vectors_dump[doc_id] for doc_id in entities_vectors_dump.files}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJdV9sh5D10P"
      },
      "outputs": [],
      "source": [
        "entities_vectors[test_doc_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0NIZRg2YfRv"
      },
      "source": [
        "### Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOdknBkcaOKj"
      },
      "outputs": [],
      "source": [
        "# DEBUG VJ pour entities – valider avec ST\n",
        "# store entities vector for each doc\n",
        "\n",
        "from collections import Counter\n",
        "entities_vectors={} # dict pour stcoker le vecteur des entités pour chaque doc\n",
        "\n",
        "count=0\n",
        "\n",
        "entities_dict={entity:i for i, entity in enumerate(Counter([y for x in entities_by_doc.values() for y in x]).keys())} # expliquer\n",
        "# print(entities_dict) # des occs à 0 – souhaité ?\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  ents=[entities_dict[x] for x in entities_by_doc[doc_id]] # mv entities entites_by_doc chargé depuis la source json (on ne calcule plus)\n",
        "  entities_vectors[doc_id]=ents\n",
        "\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)\n",
        "\n",
        "# entities_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9uWFV5JYmet"
      },
      "outputs": [],
      "source": [
        "entities_vectors[test_doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jot_BUOsY4Ug"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "import numpy as np\n",
        "np.savez('encpos_entities_vectors', **entities_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MAHIxLeHo-E"
      },
      "source": [
        "## Documents vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM_rSz4aeTNz"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgf9lCVPeY1M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "!wget https://github.com/chartes/encpos_similarities/raw/master/models/encpos_document_vectors.npz\n",
        "document_vectors_dump = np.load('encpos_document_vectors.npz')\n",
        "document_vectors = {doc_id:document_vectors_dump[doc_id] for doc_id in document_vectors_dump.files}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqgD8rmder3H"
      },
      "outputs": [],
      "source": [
        "document_vectors[test_doc_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOKCCIPIeTqu"
      },
      "source": [
        "### Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ-_r4-37n6R"
      },
      "outputs": [],
      "source": [
        "# DEBUG VJ pour document_vectors – valider avec ST\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "count=0\n",
        "document_vectors={}\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  bloc = \"\"\n",
        "  for k,v in docs_structured[doc_id].items():\n",
        "    if k!=\"metadata\":\n",
        "      bloc+=v\n",
        "    # print(v)\n",
        "  doc_embedding = model.encode([bloc])\n",
        "  document_vectors[doc_id]=doc_embedding\n",
        "\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7H4U_SFA7V-"
      },
      "outputs": [],
      "source": [
        "document_vectors.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2yljk86dX_x"
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "np.savez('encpos_document_vectors', **document_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOZwDpb_tfQ"
      },
      "source": [
        "## Doc2Vec (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0OxBAWC_4GG"
      },
      "source": [
        "### Download (`d2v_model`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkHl3kFhBM_9"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec\n",
        "d2v_model= Doc2Vec.load(\"https://github.com/chartes/encpos_similarities/raw/master/models/encpos_doc2vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YWfrshJ_-kZ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RLSQGyrAHQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e163b18-c26d-4a43-ef54-e2c61bc94e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Doc2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('french')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-j89JlXM2PP"
      },
      "outputs": [],
      "source": [
        "#transforming data\n",
        "data={k:\"\".join(list(v.values())).replace(\"=  \", \"\") for k,v in docs_structured.items()}\n",
        "data\n",
        "tagged_data = [TaggedDocument(words=[x for x in word_tokenize(v.lower()) if x not in stop_words], tags=[str(k)]) for k,v in data.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkRwEzFoWOxA"
      },
      "outputs": [],
      "source": [
        "#new method to calculate TaggedDocuments\n",
        "data={k:\"\".join([\" \".join(z) for y,z in v.items() if y!=\"metadata\" and len(z)>1]) for k,v in docs_structured_sents.items()}\n",
        "#data=[[k,\"-\"] if len(v)<50 else [k,v]]\n",
        "data={k:\"-\" if len(v)<50 else v for k,v in data.items()}\n",
        "tagged_data = [TaggedDocument(words=[x for x in word_tokenize(v.lower()) if x not in stop_words], tags=[str(k)]) for k,v in data.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6AhN7dmM2qi"
      },
      "outputs": [],
      "source": [
        "#modeling vectors, this can take a while\n",
        "max_epochs = 20\n",
        "vec_size = 30\n",
        "alpha = 0.025\n",
        "\n",
        "model_d2v = Doc2Vec(vector_size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1, window=10, workers=4)\n",
        "  \n",
        "model_d2v.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model_d2v.train(tagged_data,\n",
        "                total_examples=model_d2v.corpus_count,\n",
        "                epochs=model_d2v.iter)\n",
        "    # decrease the learning rate\n",
        "    model_d2v.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model_d2v.min_alpha = model_d2v.alpha\n",
        "\n",
        "print(\"end modelization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yTScAbXAsNF"
      },
      "outputs": [],
      "source": [
        "# Export du modèle\n",
        "model_d2v.save(\"encpos_doc2vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp_aUkHAH6Z-"
      },
      "source": [
        "## Memo Sergio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbRs_A0NM3zM"
      },
      "outputs": [],
      "source": [
        "#transforming each entities list and each text position into a vector\n",
        "#THIS IS A MANDATORY STEP AND CAN TAKE SEVERAL MINUTES\n",
        "\n",
        "from collections import Counter\n",
        "entities_vectors={}\n",
        "document_vectors={}\n",
        "keyword_vectors={}\n",
        "\n",
        "\n",
        "count=0\n",
        "\n",
        "entities_dict={entity:i for i, entity in enumerate(Counter([y for x in entities.values() for y in x]).keys())}\n",
        "\n",
        "full_text=[]\n",
        "full_text_keys=[]\n",
        "\n",
        "for item in entities.keys():\n",
        "  bloc=\"\"\n",
        "  keyword_bloc=\"\"\n",
        "  #ents=np.array([entities_dict[x] for x in entities[item]], ndmin=2)#2D array\n",
        "  ents=[entities_dict[x] for x in entities[item]]\n",
        "  for k,v in docs_structured[item].items(): # pourquoi pas sur docs_structured_sents ???\n",
        "    if k!=\"metadata\":\n",
        "      bloc+=v #join all chapters content without the title\n",
        "      \n",
        "      if len(v)>1:\n",
        "        #print(key_extractor(v))\n",
        "        try:\n",
        "          keyword_bloc+=\" \".join(key_extractor(v))+\" \"\n",
        "          \n",
        "        except:\n",
        "          continue\n",
        "  full_text.append(bloc) # on en fait quoi ? Pourquoi ?\n",
        "  full_text_keys.append(keyword_bloc)\n",
        "\n",
        "  doc_embedding = model.encode([bloc])\n",
        "  document_vectors[item]=doc_embedding\n",
        "  entities_vectors[item]=ents\n",
        "  keyword_vectors[item]=model.encode([keyword_bloc])\n",
        "  count+=1\n",
        "  if count%900==0:\n",
        "    print(count)\n",
        "\n",
        "  #print(doc_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOEOIS_tNyvB"
      },
      "source": [
        "# Similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDsIgMPh8XxI"
      },
      "outputs": [],
      "source": [
        "#cosine function to compute vectors similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cos_similarity(a,b):\n",
        "  \n",
        "  cos_sim=cosine_similarity(a, b, dense_output=False).tolist()[0][0]\n",
        "  return cos_sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e50REvvPWtyW"
      },
      "source": [
        "## Keywords vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SIvaOSXv3JR",
        "outputId": "0dcaf9a2-63b1-45d8-bbad-5622d53d1a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCPOS_1990_17\t0.84728587\t Françoise Simeray date: 1990 title: Le scriptorium et la bibliothèque de l’abbaye Saint-Amand    \n",
            "\t\t ['bibliothécaire', 'abbé', 'auteur', 'manuscrits', 'manuscrit', 'écriture', 'bibliothèques', 'libris', 'liturgiques', 'ouvrages', 'saint', 'abbaye', 'emplacement', 'église', 'bibliothèque', 'bibliques', 'siècle', 'documents'] \n",
            "\n",
            "ENCPOS_2015_12\t0.8383621\t Hélène Jacquemard date: 2015 title: L’abbaye cistercienne de Vauclair et sa bibliothèque. Lire et écrire dans une abbaye cistercienne du Moyen Âge au xviiie siècle    \n",
            "\t\t ['abbé', 'manuscrits', 'manuscrit', 'médiéval', 'libris', 'liturgiques', 'ouvrages', 'abbaye', 'livres', 'église', 'bibliothèque', 'clairvaux', 'liste', 'fonds', 'abbés', 'documents'] \n",
            "\n",
            "ENCPOS_1969_09\t0.794045\t Marie-Pierre Laffitte-Pochat date: 1969 title: La bibliothèque et le scriptorium de Saint-Thierry de Reims    \n",
            "\t\t ['vatican', 'textes', 'abbé', 'médiévales', 'manuscrits', 'manuscrit', 'écriture', 'inventaires', 'lettres', 'libris', 'liturgiques', 'ouvrages', 'catalogue', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'fonds', 'siècle', 'abbés', 'bible'] \n",
            "\n",
            "ENCPOS_1998_35\t0.79267997\t Laurent Veyssière date: 1998 title: Recueil des chartes de l’abbaye de Clairvaux.    \n",
            "\t\t ['bibliothécaire', 'médiévale', 'abbé', 'manuscrits', 'sources', 'liturgiques', 'moine', 'abbaye', 'cisterciennes', 'bibliothèque', 'clairvaux', 'fonds', 'bernard'] \n",
            "\n",
            "ENCPOS_2009_25\t0.78906024\t Cécile Roger date: 2009 title: Guillaume de Saint-Lô, un prédicateur à l’œuvre au xive siècle    \n",
            "\t\t ['bibliothécaire', 'textes', 'table', 'abbé', 'manuscrits', 'manuscrit', 'sources', 'liturgiques', 'auteurs', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'bibliques', 'documents'] \n",
            "\n",
            "ENCPOS_2010_23\t0.7857414\t Cécile Roger date: 2010 title: Guillaume de Saint-Lô un prédicateur à l’œuvre au xive siècle    \n",
            "\t\t ['bibliothécaire', 'textes', 'table', 'abbé', 'manuscrit', 'sources', 'liturgiques', 'auteurs', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'bibliques', 'documents'] \n",
            "\n",
            "ENCPOS_2012_13\t0.774467\t Pauline Gendry date: 2012 title: Liturgie et vie monastique à l’abbaye bénédictine Saint-Martin de Savigny d’après son ordinaire médiéval. Édition critique et commentaire du ms. Archives départementales du Rhône, 1 H 20 (deuxième quart du xiiie siècle)    \n",
            "\t\t ['médiévale', 'abbé', 'manuscrit', 'liturgiques', 'saint', 'moine', 'abbaye', 'église', 'bibliothèque'] \n",
            "\n",
            "ENCPOS_1913_02\t0.7711915\t Eugène Berger date: 1913 title: Étude historique et archéologique sur l’abbaye de Saint-Père de Chartres    \n",
            "\t\t ['abbé', 'manuscrits', 'sources', 'saint', 'abbaye', 'église', 'bibliothèque', 'siècle', 'abbés'] \n",
            "\n",
            "ENCPOS_1913_12\t0.7698721\t Joseph Macquart De Terline date: 1913 title: Étude sur l’abbaye de Cercamp (ordre de Cîteaux, diocèse d’Amiens)    \n",
            "\t\t ['abbé', 'manuscrits', 'pontigny', 'saint', 'abbaye', 'église', 'liste', 'abbés', 'documents'] \n",
            "\n",
            "ENCPOS_1979_01\t0.7685126\t Françoise Bérard date: 1979 title: La bibliothèque des Célestins de Paris d’après le catalogue collectif du Père Daire (vers 1767-1776) et les manuscrits retrouvés    \n",
            "\t\t ['bibliothécaire', 'manuscrits', 'manuscrit', 'inventaires', 'paris', 'bibliothèques', 'libris', 'auteurs', 'ouvrages', 'catalogue', 'saint', 'livres', 'église', 'bibliothèque', 'liste', 'fonds', 'siècle'] \n",
            "\n",
            "ENCPOS_2007_21\t0.76798695\t Laure Rioust date: 2007 title: La Bible de Saint-André-au-Bois. (Boulogne-sur-Mer, BM, ms. 2)    \n",
            "\t\t ['textes', 'abbé', 'biblique', 'manuscrits', 'manuscrit', 'parisien', 'pontigny', 'lettres', 'liturgiques', 'bois', 'catalogue', 'saint', 'abbaye', 'fonds', 'bible'] \n",
            "\n",
            "ENCPOS_1920b_01\t0.76131207\t André Courtet date: 1920 title: Étude historique sur l’évolution de l’abbaye de Pontigny    \n",
            "\t\t ['abbé', 'pontigny', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'siècle', 'abbés'] \n",
            "\n",
            "ENCPOS_2006_03\t0.75657165\t Gordon Blennemann date: 2006 title: Le nécrologe du livre du chapitre de l’abbaye Saint-Pierre-aux-Nonnains de Metz (BNF lat. 10028). Commentaire et édition    \n",
            "\t\t ['manuscrit', 'sources', 'font', 'saint', 'abbaye', 'pierre', 'bibliothèque', 'liste'] \n",
            "\n",
            "ENCPOS_1953_17\t0.75525147\t Geneviève Nortier-Marchand date: 1953 title: Les bibliothèques médiévales des abbayes bénédictines de Normandie    \n",
            "\t\t ['abbé', 'manuscrits', 'bibliothèques', 'liturgiques', 'ouvrages', 'catalogue', 'saint', 'abbaye', 'livres', 'bibliothèque', 'liste', 'siècle', 'abbés', 'documents'] \n",
            "\n",
            "ENCPOS_1969_12\t0.75446147\t Catherine Marion date: 1969 title: Le temporel de l’abbaye Saint-Paul de Besançon des origines à 1333     \n",
            "\t\t ['abbé', 'saint', 'abbaye', 'église', 'liste', 'siècle', 'abbés'] \n",
            "\n",
            "ENCPOS_1964_06\t0.7530987\t Françoise Canut date: 1964 title: Le cartulaire de l’abbaye de Signy au diocèse de Reims, ordre de Cîteaux    \n",
            "\t\t ['médiévale', 'manuscrits', 'manuscrit', 'sources', 'écriture', 'catalogue', 'saint', 'abbaye', 'bibliothèque', 'liste', 'siècle', 'abbés'] \n",
            "\n",
            "ENCPOS_1963_03\t0.75084263\t François Avril date: 1963 title: La décoration des manuscrits dans les abbayes bénédictines de Normandie aux xie et xiie siècles    \n",
            "\t\t ['abbé', 'manuscrits', 'manuscrit', 'bibliothèques', 'liturgiques', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'abbés', 'bible'] \n",
            "\n",
            "ENCPOS_2003_20\t0.7501037\t Raphaël Richter date: 2003 title: Édition de 23 sermons du manuscrit autographe d’Adémar de Chabannes, BNF lat. 2469, ff. 1-76 ( ca.1030). Martial et Adémar.    \n",
            "\t\t ['manuscrits', 'manuscrit', 'écriture', 'edition', 'littéraire', 'saint', 'abbaye', 'église'] \n",
            "\n",
            "ENCPOS_1968_10\t0.74841726\t Henri Guy date: 1968 title: Le cartulaire de l’abbaye du Mont-Sainte-Catherine-lès-Provins, ordre de sainte Claire    \n",
            "\t\t ['table', 'manuscrit', 'libris', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'documents'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#similarity using KEYWORDS vectors\n",
        "search_term=keywords_vectors[test_doc_id]\n",
        "\n",
        "similar_terms=[[k, cosine_similarity(search_term, v)[0][0], docs_structured[k][\"metadata\"].split(\"creator:\")[1] ] for k,v in keywords_vectors.items() if len(docs_structured[k])>1]\n",
        "similar_terms=sorted(similar_terms, key = lambda x: x[1], reverse=True)[:20]\n",
        "for x in similar_terms:\n",
        "  if x[0]!=test_doc_id:\n",
        "    shared_keywords = [y for y in set(keywords_by_doc[test_doc_id].split()) if y in set(keywords_by_doc[x[0]].split()) ]\n",
        "    #x.append(sorted(shared_keywords)) # ajout des keywords à la liste\n",
        "    print(*x, sep='\\t')\n",
        "    print('\\t\\t', shared_keywords, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHgNTE4SWx4B"
      },
      "source": [
        "## Entities lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaGwpn6Z5Fty",
        "outputId": "13242d44-1266-4dde-a95d-ed4a3c073f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCPOS_1922_01 0.1428571  André Barroux date: 1922 title: Essai sur le guet ordinaire à Paris. Son organisation de 1364 à 1559. Son évocation depuis la réforme de 1559 jusqu’à l’établissement des « cavaliers de l’ordonnance » en 1666     \n",
            "\t\t {'Colbert', 'Édit du mois', 'Bureau de la ville', 'Delamare', 'Paris', 'roi Jean', 'Traité de la Police'} \n",
            "\n",
            "ENCPOS_1986_15 0.1395349  Caroline Obert-Piketty date: 1986 title: Les maîtres et étudiants du collège Saint-Bernard de Paris de 1224 à 1494     \n",
            "\t\t {'abbé de Clairvaux', 'Jean de Cirey', 'Moyen Age', 'Benoît XII', 'Paris', 'Pontigny', 'Innocent IV', 'Cisterciens', 'collège Saint-Bernard', 'Toulouse', 'Cîteaux', 'Estella', 'Occident', 'Montpellier', 'Jean Tolet', 'Oxford', 'Morimond', 'Clairvaux', 'Étienne de Lexington', 'Belgique', 'Alphonse de Poitiers', 'ordre de Cîteaux'} \n",
            "\n",
            "ENCPOS_1970_11 0.1363636  Denis Escudier date: 1970 title: Le scriptorium de Saint-Vaast d’Arras des origines au xiie siècle. Contribution à l’étude des notations neumatiques du nord de la France     \n",
            "\t\t {'Robert de Molesme', 'Albertus', 'Bibliothèque Royale de Belgique', 'saint Vaast', 'Paris', 'Saint-Vaast', 'Grande Bible', 'saint Jérôme', 'Douai', 'Bruxelles', 'Arras', '9850-9852', 'Dijon', 'Cîteaux', 'Commentaire', 'Cambrai', 'Lamentations', 'Liber miraculorum', 'Oisbertus', 'Etienne Harding', 'Vitae Patrum', 'Arras 559', 'Jérémie'} \n",
            "\n",
            "ENCPOS_1968_07 0.1363636  Anne-Marie Enaux-Moret date: 1968 title: Abel Servien, négociateur des traités de Wesphalie, du Parlement de Grenoble à la surintenance des finances (1593-1659)     \n",
            "\t\t {'Mazarin', 'Longueville', 'Chavigny', 'Condé', 'Affaires étrangères', 'Paris', 'Anjou', 'Poitiers', 'Colbert', 'Baluze', 'Münster', 'comte d’Avaux', 'Bibliothèque nationale', 'union des Frondes', 'l’Empire', 'Conseil', 'Cinq-cents', 'Richelieu', 'Cherasco', 'Servien', 'Abel Servien', 'Dupuy', 'Archives nationales'} \n",
            "\n",
            "ENCPOS_1988_10 0.1333333  Patricia Gillet date: 1988 title: Étienne Baluze (1630-1718) et l’histoire du Limousin : méthodes et desseins d’un érudit du xviie siècle     \n",
            "\t\t {'Maison d’Auvergne', 'Moyen Age', 'Pompadour', 'Limoges', 'Adhémar de Chabannes', 'Miscellanea', 'Paris', 'Obazine, Beaulieu', 'Meymac', 'Colbert', 'abbayes de Dalon', 'Armand Gérard', 'Vigeois', 'Baluze', 'Toulouse', 'Gaignières', 'Mabillon', 'Turenne', 'Limousin', 'Bas-Limousin', 'Tulle', 'Uzerche', 'St-Martin', 'Hérouval'} \n",
            "\n",
            "ENCPOS_2001_11 0.1333333  Laure Jestaz date: 2001 title: Édition critique de lettres de Guy Patin (Bibl. nat. de France, Baluze 148)     \n",
            "\t\t {'Faculté de médecine de Paris', 'Louis XIII', 'Anne d’Autriche', 'Mazarin', 'Paris', 'Colbert', 'Baluze', 'révolte du', 'Patin', 'Faculté de Paris', 'Jean Pecquet', 'William Harvey', 'Languedoc', 'guerre de Trente Ans', 'France', 'Charles Spon', 'Parlement', 'rébellion de Gaston d’Orléans', 'Gaston d’Orléans', 'Richelieu', 'Faculté de Montpellier', 'Théophraste Renaudot', 'la France', 'Guy Patin'} \n",
            "\n",
            "ENCPOS_1901_02 0.1333333  René Bonnat date: 1901 title: Nicolas de La Reynie, premier lieutenant de police     \n",
            "\t\t {'Châtelet', 'Mignard', 'Chapelain', 'Françoise Nicolas', 'Jean Nicolas de Tralage', 'Gabriel Nicolas de La Reynie', 'Bordeaux', 'Limoges', 'Tralage', 'Paris', 'Jean Nicolas', 'Colbert', 'Saint-Simon', 'Baluze', 'Françoise de Sainte-Thérèse', 'Sagot', 'La Reynie', 'Gaudion', 'Louvois', 'Parlement', 'Coudray', 'Seignelay', '– Gabriel', 'Guy Patin'} \n",
            "\n",
            "ENCPOS_1977_03 0.1333333  Jacques Berlioz date: 1977 title: Le Tractatus de diversis materiis predicabilibus d’Étienne de Bourbon. Troisième partie : De dono scientie. Étude et édition     \n",
            "\t\t {'Moralia in Job', 'Paris', 'saint Jérôme', 'Beaujolais', 'Enarrationes in Psalmos', 'Pères de l’Église', 'Dombes', 'Bourgogne', 'Champagne', 'saint Grégoire', 'Piémont', 'Tractatus', 'falsa poenitentia', 'Massif central', 'Forez', 'Étienne de Bourbon', 'Humbert de Romans', 'saint Ambroise', 'Savoie', 'De vera', 'saint Isidore de Séville', 'saint Augustin', 'Pseudo-Augustin', 'Homeliae in Evangelia'} \n",
            "\n",
            "ENCPOS_1965_11 0.1333333  Odile Tran-Thuan-Biausse date: 1965 title: Recueil des plus anciens actes de l’abbaye d’Yerres (1132-1265)     \n",
            "\t\t {'Delisle', 'Yerres', 'Paris', 'Baluze', 'abbé Alliot', 'Q115071-3', 'Bibliothèque du roi', 'Gaignières', 'Mabillon', 'Lasteyrie', 'Cîteaux', 'Saint-Jacques', 'abbé Lebeuf', 'roi de France', 'Antoine Véronneau', 'Louis VI', 'Archives nationales', 'Étienne de Senlis', 'Etienne de Senlis', 'Archives de Seine-et-Oise', 'Eustachie de Corbeil', 'Luchaire', 'Saint-Nicolas', 'K 179'} \n",
            "\n",
            "ENCPOS_1973_12 0.1304348  Cécile Eymard date: 1973 title: Les Marguerites Hystorialles de Jehan Massue. Édition critique     \n",
            "\t\t {'Sénèque', 'Valère', 'Lucain', 'Chroniques de France', 'Quinte-Curce', 'Boccace', 'Mer des Histoires', 'Virgile', 'saint Jérôme', 'Hugues de Saint-Victor', 'Justin', 'Pline', 'Tite-Live', 'Cicéron', 'Jehan Massue', 'Eutrope', 'Chronique martiniane', 'Aristote', 'Jean de Chabannes', 'Fabius Pictor', 'Suétone', 'saint Ambroise', 'Salluste', 'saint Augustin', 'Orose'} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Similarity using ENTITIES vectors\n",
        "cherche=entities_by_doc[test_doc_id]\n",
        "similar_docs_number = 10\n",
        "similar_docs = []\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  try:\n",
        "    coeff=(2*len(list(set(cherche) & set(entities_by_doc[doc_id]))))/(len(cherche)+len(entities_by_doc[doc_id]))\n",
        "    if test_doc_id == doc_id:\n",
        "      continue\n",
        "    if coeff>0.05:\n",
        "      similar_docs.append([round(coeff,7), doc_id, entities_by_doc[doc_id]])\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "similar_docs.sort(key=lambda x:x[0], reverse=True)\n",
        "#similar_docs[0:similar_docs_number]\n",
        "for l in similar_docs[0:similar_docs_number]:\n",
        "  print(l[1], l[0], docs_structured[l[1]][\"metadata\"].split(\"creator:\")[1], '\\n\\t\\t', set(l[2]), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcxWALkAW3En"
      },
      "source": [
        "## Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_dsvkYGhX2y",
        "outputId": "9e0f97ef-8c37-4a23-8973-1e9882721873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCPOS_1986_07\t0.58974755\t Thibaut Girard date: 1986 title: Le Traité du droit et du comportement des armes de Du Cange. Édition critique et commentaire    \n",
            "ENCPOS_2008_12\t0.54115784\t Stéphanie Deprouw date: 2008 title: Un héritage des Bonaparte : Le prix du galvanisme (1802-1815) et le prix Volta (1852-1888). L’État et l’encouragement à la recherche sur l’électricité    \n",
            "ENCPOS_1975_13\t0.5291034\t Anita Guerreau-Jalabert date: 1975 title: Grammaire et culture profane à Fleury au xe siècle. Les Quaestiones grammaticales d’Abbon de Fleury    \n",
            "ENCPOS_1981_18\t0.50414956\t Annick Notter date: 1981 title: Le culte des saints en Sologne aux xixe et xxe siècles    \n",
            "ENCPOS_1998_18\t0.49594107\t Isabelle Homer date: 1998 title: Médecins et chirurgiens à Saint-Domingue au xviiie siècle    \n",
            "ENCPOS_1997_37\t0.4866734\t Nicolas Roche date: 1997 title: Les armoiries imaginaires des personnages de l’Antiquité, de l’Orient et de la Bible (xiie-xviie siècle)    \n",
            "ENCPOS_2011_01\t0.47874796\t Justine Ancelin date: 2011 title: Édition critique et étude du Journal de la vie privée de Jean-Dominique Cassini (1710-1712). Science, académisme et sociabilité savante    \n",
            "ENCPOS_2009_14\t0.4767804\t Philippe Galanopoulos date: 2009 title: L’enseignement de l’histoire de la médecine à Paris au xixe siècle (1794-1914). La défaite de l’érudition    \n",
            "ENCPOS_2009_13\t0.46876967\t Charles-Antoine Fogielman-Curley date: 2009 title: La pratique de l’épigraphie dans l’ordre de Cluny (909-1300)    \n"
          ]
        }
      ],
      "source": [
        "#similarity using DOCUMENT vectors\n",
        "searched_doc_vec=document_vectors[test_doc_id]\n",
        "similar_docs = []\n",
        "'''\n",
        "for k,v in document_vectors.items():\n",
        "  cos=cosine_similarity(searched_doc_vec, v)\n",
        "  if cos>0.45:\n",
        "    print(k, \"\\t\", cos[0], \"\\t\", docs_structured[k][\"metadata\"].split(\"creator:\")[1])\n",
        "'''\n",
        "similar_docs=[[k, cosine_similarity(searched_doc_vec, v)[0][0], docs_structured[k][\"metadata\"].split(\"creator:\")[1] ] for k,v in document_vectors.items() if len(docs_structured[k])>1]\n",
        "similar_docs=sorted(similar_docs, key = lambda x: x[1], reverse=True)[:10]\n",
        "for x in similar_docs:\n",
        "  if x[0]!=test_doc_id: # todo\n",
        "    print(*x, sep='\\t')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZqLzbzeW8x2"
      },
      "source": [
        "## Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd43NHlEQDi_",
        "outputId": "d1a5638b-0eb4-41cc-9d18-d367208f5b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCPOS_1933_13 0.46514  Régine Pernoud date: 1933 title: Essai sur le port de Marseille des origines à la fin du xiiie siècle    \n",
            "ENCPOS_1971_12 0.46075  Alain Guerreau date: 1971 title: Une ville et ses finances : Mâcon    \n",
            "ENCPOS_1994_04 0.37603  Michelle Bubenicek date: 1994 title: Le pouvoir au féminin. Une princesse en politique et son entourage : Yolande de Flandre, comtesse de Bar et dame de Cassel (1326-1395)    \n",
            "ENCPOS_1999_35 0.37088  Elsa Marguin date: 1999 title: L’Ars lectoria Ecclesie de Jean de Garlande. Étude, édition et traduction    \n",
            "ENCPOS_1971_15 0.3624  Danielle Jacquart date: 1971 title: Un médecin parisien du xve siècle, Jacques Despars (1380-1458)    \n",
            "ENCPOS_1971_25 0.35464  Jean-Claude Schmitt date: 1971 title: L’Église et les clercs face aux béguines et aux beghards du Rhin supérieur du xive siècle au xve siècle    \n",
            "ENCPOS_2000_08 0.33392  Olivier Canteaut date: 2000 title: Philippe V et son Conseil : le gouvernement royal de 1316 à 1322    \n",
            "ENCPOS_2003_19 0.32007  Sébastien Nadiras date: 2003 title: Guillaume de Nogaret et la pratique du pouvoir    \n",
            "ENCPOS_2002_29 0.31963  Dominique Stutzmann date: 2002 title: La bibliothèque de l’abbaye cistercienne de Fontenay (Côte-d’Or). Constitution, gestion, dissolution (xiie-xviiie siècle).    \n"
          ]
        }
      ],
      "source": [
        "#similarity using Doc2Vec model\n",
        "searched_doc = test_doc_id\n",
        "similar_docs = d2v_model.docvecs.most_similar(searched_doc, topn=15)\n",
        "for x in similar_docs:\n",
        "  try:\n",
        "    print(x[0], round(x[1], 5), docs_structured[x[0]][\"metadata\"].split(\"creator:\")[1])\n",
        "  except:\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search by term"
      ],
      "metadata": {
        "id": "5nVuLQImc-gV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "YdUaJyaNwQsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93dd4bd6-6fbd-4577-f279-4bffd75e3d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCPOS_1991_08\t[0.3652228]\t Sylvie Fayet date: 1991 title: L’expression de la couleur dans les textes littéraires latins du xiie siècle. Contribution au lexique et éléments d’un imaginaire    \n",
            "ENCPOS_1995_35\t[0.25018558]\t Inès Villela-Petit date: 1995 title: La peinture médiévale vers 1400, autour d’un manuscrit de Jean Lebègue. Édition des Libri colorum    \n",
            "ENCPOS_2017_07\t[0.17699459]\t Julie Duprat date: 2017 title: Présences noires à Bordeaux : passage et intégration des gens de couleur à la fin du xviiie siècle    \n",
            "ENCPOS_2015_17\t[0.10069939]\t Thomas Morel date: 2015 title: Quand un prélat monte à la capitale. Tristan de Salazar et l’hôtel parisien des archevêques de Sens    \n",
            "ENCPOS_1922_11\t[0.09992971]\t Henry Joly date: 1922 title: L’expédition de la Corse (1553-1559). Épisode de la rivalité franco-espagnole dans la Méditerranée occidentale    \n",
            "ENCPOS_1964_08\t[0.09765615]\t Lise Dupouy date: 1964 title: Les protestants de Florac de la révocation de l’Édit de Nantes à l’Édit de tolérance (1685-1787)    \n",
            "ENCPOS_1987_18\t[0.08728624]\t Françoise Malet date: 1987 title: Un royaliste au service de la légitimité : Jean-Guillaume Hyde de Neuville (1776-1857)    \n",
            "ENCPOS_2007_04\t[0.07919207]\t Thibaud Boüard date: 2007 title: Le Trésor de Brunetto Latini. Éduquer les Laïcs    \n",
            "ENCPOS_1984_10\t[0.07916784]\t Guy-Michel Leproux date: 1984 title: Recherches sur les peintres-verriers parisiens de la Renaissance (1540-1620)    \n",
            "ENCPOS_1947_09\t[0.07906316]\t Chantal de La Véronne date: 1947 title: Histoire de la ville du Blanc, des origines à 1789    \n",
            "ENCPOS_1950_05\t[0.07757609]\t Gilles Caster date: 1950 title: Le commerce du pastel à Toulouse au xvie siècle    \n",
            "ENCPOS_1979_05\t[0.07587212]\t Nicole Garnier date: 1979 title: Antoine Coypel (1661-1722). L’homme et l’œuvre    \n",
            "ENCPOS_2006_01\t[0.07194686]\t Jean-Pierre Bat date: 2006 title: Congo an I. Décolonisation et politique francaise au Congo-Brazzaville (1958-1963)    \n",
            "ENCPOS_1900_05\t[0.07131065]\t Paul Denis date: 1900 title: Le droit de gave de Cambrésis, étude de la protection des princes comtes de Flandre sur l’évêché et les églises de Cambrai (1144-1687)    \n",
            "ENCPOS_1999_06\t[0.06972058]\t Caroline Becker-Jeanjean date: 1999 title: Les récits illustrés de voyages pittoresques publiés en France entre 1770 et 1855    \n",
            "ENCPOS_1929_18\t[0.06650721]\t Germaine Pastré date: 1929 title: Les seigneurs de Florac du début du xiiie à la fin du xive siècle    \n",
            "ENCPOS_1951_19\t[0.06588609]\t Marie-Henriette de Montéty date: 1951 title: Albert de Gondi, maréchal de Retz (1522-1602)    \n",
            "ENCPOS_1901_07\t[0.06280689]\t Alfred Gandilhon date: 1901 title: Essai sur la vie privée et la cour de Louis XI (1461-1483)    \n",
            "ENCPOS_1932_01\t[0.06191678]\t Jean Adhémar date: 1932 title: L’antiquité classique dans l’art du Moyen Âge français. Études sur les sources d’inspiration plastiques et littéraires    \n",
            "ENCPOS_2020_16\t[0.06070822]\t Pia Rigaldiès date: 2020 title: Italie, design et politique : fabrique d’un modèle et transferts culturels vers la France (1964-début des années 1990). Étude fondée sur les archives de Gruppo Strum et Studio 65 à Turin    \n"
          ]
        }
      ],
      "source": [
        "cherche=model.encode([\"couleur\"]) # musique, amour, mort, charte, animal, sport, couleur\n",
        "related_docs = []\n",
        "\n",
        "related_docs=[[k, cosine_similarity(cherche, v)[0], docs_structured[k][\"metadata\"].split(\"creator:\")[1] ] for k,v in document_vectors.items() if len(docs_structured[k])>1]\n",
        "related_docs=sorted(related_docs, key = lambda x: x[1], reverse=True)[:20]\n",
        "for x in related_docs:\n",
        "  print(*x, sep='\\t')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search by number of common entities\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Oo63xDBxt4QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search by docs, using shared entities\n",
        "cherche=entities_by_doc[test_doc_id]\n",
        "related_docs = []\n",
        "\n",
        "for doc_id in entities_by_doc.keys():\n",
        "  try:\n",
        "    a=[x for x in cherche if x in entities_by_doc[doc_id]]\n",
        "    related_docs.append([doc_id, len(a), a, docs_structured[doc_id][\"metadata\"].split(\"creator:\")[1]])\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "related_docs=sorted(related_docs, key = lambda x: x[1], reverse=True)[:20]   \n",
        "for x in related_docs:\n",
        "  if test_doc_id!=x[0]:\n",
        "    print(*x, sep='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7jNPHqtptB-",
        "outputId": "83233a4e-869a-43a7-a550-216e49d83c83"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCPOS_1901_02\t3\t['Paris', 'Baluze', 'Colbert']\t René Bonnat date: 1901 title: Nicolas de La Reynie, premier lieutenant de police    \n",
            "ENCPOS_1965_11\t3\t['Paris', 'Cîteaux', 'Baluze']\t Odile Tran-Thuan-Biausse date: 1965 title: Recueil des plus anciens actes de l’abbaye d’Yerres (1132-1265)    \n",
            "ENCPOS_1970_11\t3\t['Paris', 'Cîteaux', 'saint Jérôme']\t Denis Escudier date: 1970 title: Le scriptorium de Saint-Vaast d’Arras des origines au xiie siècle. Contribution à l’étude des notations neumatiques du nord de la France    \n",
            "ENCPOS_1986_15\t3\t['Clairvaux', 'Paris', 'Cîteaux']\t Caroline Obert-Piketty date: 1986 title: Les maîtres et étudiants du collège Saint-Bernard de Paris de 1224 à 1494    \n",
            "ENCPOS_1953_16\t3\t['Paris', 'Cîteaux', 'saint Bernard']\t Jean-François Maurel date: 1953 title: Jean Beleth et la Summa de ecclesiasticis officiis    \n",
            "ENCPOS_1973_12\t3\t['saint Ambroise', 'saint Jérôme', 'Hugues de Saint-Victor']\t Cécile Eymard date: 1973 title: Les Marguerites Hystorialles de Jehan Massue. Édition critique    \n",
            "ENCPOS_2001_11\t3\t['Paris', 'Baluze', 'Colbert']\t Laure Jestaz date: 2001 title: Édition critique de lettres de Guy Patin (Bibl. nat. de France, Baluze 148)    \n",
            "ENCPOS_1968_07\t3\t['Paris', 'Baluze', 'Colbert']\t Anne-Marie Enaux-Moret date: 1968 title: Abel Servien, négociateur des traités de Wesphalie, du Parlement de Grenoble à la surintenance des finances (1593-1659)    \n",
            "ENCPOS_1977_03\t3\t['Paris', 'saint Ambroise', 'saint Jérôme']\t Jacques Berlioz date: 1977 title: Le Tractatus de diversis materiis predicabilibus d’Étienne de Bourbon. Troisième partie : De dono scientie. Étude et édition    \n",
            "ENCPOS_1988_10\t3\t['Paris', 'Baluze', 'Colbert']\t Patricia Gillet date: 1988 title: Étienne Baluze (1630-1718) et l’histoire du Limousin : méthodes et desseins d’un érudit du xviie siècle    \n",
            "ENCPOS_1997_08\t2\t['Clairvaux', 'saint Bernard']\t Stéphanie Billot date: 1997 title: Trois-Fontaine, fille aînée de Clairvaux : étude et édition du chartrier (1118-1231)    \n",
            "ENCPOS_1949_24\t2\t['saint Bernard', 'saint Ambroise']\t Bernadette Terris date: 1949 title: La Vierge de la Passion dans la littérature jusqu’à la fin du xve siècle    \n",
            "ENCPOS_2002_25\t2\t['Paris', 'Colbert']\t Emmanuel Penicaut date: 2002 title: Michel Chamillart, ministre et secrétaire d’État de la guerre de Louis XIV (1654-1721)    \n",
            "ENCPOS_1964_01\t2\t['Paris', 'Colbert']\t Anne Bondeelle-Souchier date: 1964 title: La population de Sceaux de 1609 à 1749    \n",
            "ENCPOS_1922_01\t2\t['Paris', 'Colbert']\t André Barroux date: 1922 title: Essai sur le guet ordinaire à Paris. Son organisation de 1364 à 1559. Son évocation depuis la réforme de 1559 jusqu’à l’établissement des « cavaliers de l’ordonnance » en 1666    \n",
            "ENCPOS_1975_14\t2\t['saint Ambroise', 'saint Jérôme']\t Catherine Gutowski date: 1975 title: Le traité De Avaricia extrait de la Summa de Viciis de Guillaume Peyraut. Édition critique partielle et commentaire    \n",
            "ENCPOS_1911_04\t2\t['Paris', 'Colbert']\t Maximilien Courtecuisse date: 1911 title: La manufacture de draps fins Vanrobais au xviie et au xviiie siècle    \n",
            "ENCPOS_1985_10\t2\t['Paris', 'Hugues de Saint-Victor']\t Isabelle Le Masne De Chermont date: 1985 title: Le recueil des sermons de Frédéric Visconti, archevêque de Pise de 1254 à 1277. Édition critique et commentaire du sanctoral    \n",
            "ENCPOS_1958_16\t2\t['saint Bernard', 'saint Ambroise']\t Marie-Ange Palewska date: 1958 title: Recherches sur le Bonum universale de apibus de Thomas de Cantimpré, suivies de l’édition des exempla d’après la traduction française faite pour Charles V en 1372    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search by number of common keywords"
      ],
      "metadata": {
        "id": "TqaWsjR6xlLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "U-kT7in4Ofuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb04106-2b43-46b7-d0ea-7ac9183da89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCPOS_1969_09\t22\t Marie-Pierre Laffitte-Pochat date: 1969 title: La bibliothèque et le scriptorium de Saint-Thierry de Reims    \t['vatican', 'textes', 'abbé', 'médiévales', 'manuscrits', 'manuscrit', 'écriture', 'inventaires', 'lettres', 'libris', 'liturgiques', 'ouvrages', 'catalogue', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'fonds', 'siècle', 'abbés', 'bible']\n",
            "ENCPOS_1975_14\t18\t Catherine Gutowski date: 1975 title: Le traité De Avaricia extrait de la Summa de Viciis de Guillaume Peyraut. Édition critique partielle et commentaire    \t['médiévale', 'biblique', 'auteur', 'manuscrits', 'médiéval', 'bibliothèques', 'littéraire', 'œuvres', 'littérature', 'auteurs', 'ouvrages', 'catalogue', 'église', 'bibliothèque', 'bibliophile', 'bibliques', 'siècle', 'bible']\n",
            "ENCPOS_2001_18\t18\t Karine Rebmeister date: 2001 title: La bibliothèque médiévale du collège des Cholets    \t['médiévale', 'manuscrits', 'sources', 'edition', 'médiéval', 'paris', 'bibliothèques', 'libris', 'liturgiques', 'auteurs', 'ouvrages', 'catalogue', 'livres', 'emplacement', 'bibliothèque', 'fonds', 'siècle', 'documents']\n",
            "ENCPOS_1990_17\t18\t Françoise Simeray date: 1990 title: Le scriptorium et la bibliothèque de l’abbaye Saint-Amand    \t['bibliothécaire', 'abbé', 'auteur', 'manuscrits', 'manuscrit', 'écriture', 'bibliothèques', 'libris', 'liturgiques', 'ouvrages', 'saint', 'abbaye', 'emplacement', 'église', 'bibliothèque', 'bibliques', 'siècle', 'documents']\n",
            "ENCPOS_1978_08\t18\t Nicole Guibout-Chagué date: 1978 title: Les manuscrits des Jacobins de la rue Saint-Jacques d’après les inventaires révolutionnaires dressés pour les dépôts littéraires    \t['textes', 'manuscrits', 'manuscrit', 'inventaires', 'paris', 'bibliothèques', 'littéraire', 'libris', 'auteurs', 'ouvrages', 'catalogue', 'saint', 'abbaye', 'livres', 'dépôts', 'bibliothèque', 'documents', 'bible']\n",
            "ENCPOS_1979_01\t17\t Françoise Bérard date: 1979 title: La bibliothèque des Célestins de Paris d’après le catalogue collectif du Père Daire (vers 1767-1776) et les manuscrits retrouvés    \t['bibliothécaire', 'manuscrits', 'manuscrit', 'inventaires', 'paris', 'bibliothèques', 'libris', 'auteurs', 'ouvrages', 'catalogue', 'saint', 'livres', 'église', 'bibliothèque', 'liste', 'fonds', 'siècle']\n",
            "ENCPOS_2015_12\t16\t Hélène Jacquemard date: 2015 title: L’abbaye cistercienne de Vauclair et sa bibliothèque. Lire et écrire dans une abbaye cistercienne du Moyen Âge au xviiie siècle    \t['abbé', 'manuscrits', 'manuscrit', 'médiéval', 'libris', 'liturgiques', 'ouvrages', 'abbaye', 'livres', 'église', 'bibliothèque', 'clairvaux', 'liste', 'fonds', 'abbés', 'documents']\n",
            "ENCPOS_2009_25\t16\t Cécile Roger date: 2009 title: Guillaume de Saint-Lô, un prédicateur à l’œuvre au xive siècle    \t['bibliothécaire', 'textes', 'table', 'abbé', 'manuscrits', 'manuscrit', 'sources', 'liturgiques', 'auteurs', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'bibliques', 'documents']\n",
            "ENCPOS_2007_21\t15\t Laure Rioust date: 2007 title: La Bible de Saint-André-au-Bois. (Boulogne-sur-Mer, BM, ms. 2)    \t['textes', 'abbé', 'biblique', 'manuscrits', 'manuscrit', 'parisien', 'pontigny', 'lettres', 'liturgiques', 'bois', 'catalogue', 'saint', 'abbaye', 'fonds', 'bible']\n",
            "ENCPOS_1985_04\t15\t Isabelle Chiavassa-Gouron date: 1985 title: Les lectures des maîtres et étudiants du collège de Navarre : un aspect de la vie intellectuelle à l’Université de Paris (1380-1520)    \t['bibliothécaire', 'textes', 'manuscrits', 'bibliothèques', 'littéraire', 'libris', 'liturgiques', 'auteurs', 'pierre', 'livres', 'église', 'bibliothèque', 'bibliques', 'fonds', 'bible']\n",
            "ENCPOS_1986_15\t15\t Caroline Obert-Piketty date: 1986 title: Les maîtres et étudiants du collège Saint-Bernard de Paris de 1224 à 1494    \t['abbé', 'manuscrits', 'manuscrit', 'sources', 'parisien', 'paris', 'ouvrages', 'saint', 'abbaye', 'livres', 'bibliothèque', 'clairvaux', 'abbés', 'documents', 'bible']\n",
            "ENCPOS_2010_23\t15\t Cécile Roger date: 2010 title: Guillaume de Saint-Lô un prédicateur à l’œuvre au xive siècle    \t['bibliothécaire', 'textes', 'table', 'abbé', 'manuscrit', 'sources', 'liturgiques', 'auteurs', 'saint', 'abbaye', 'église', 'bibliothèque', 'liste', 'bibliques', 'documents']\n",
            "ENCPOS_1970_04\t15\t Christine Bischoff date: 1970 title: L’Hortus deliciarum d’Herrade de Landsberg : essai de reconstitution du texte    \t['textes', 'table', 'auteur', 'manuscrits', 'manuscrit', 'sources', 'écriture', 'littéraire', 'œuvres', 'auteurs', 'ouvrages', 'abbaye', 'église', 'bibliothèque', 'siècle']\n",
            "ENCPOS_1958_20\t15\t Jean Vezin date: 1958 title: Les scriptoria d’Angers au xie siècle    \t['textes', 'abbé', 'manuscrits', 'manuscrit', 'écriture', 'lettres', 'libris', 'ouvrages', 'catalogue', 'saint', 'abbaye', 'livres', 'bibliothèque', 'siècle', 'bible']\n",
            "ENCPOS_2016_14\t15\t Émeline Pipelier date: 2016 title: « Utile, si je puis ». Les ouvrages d’éducation de Pierre Blanchard (1809-1832)    \t['auteur', 'sources', 'parisien', 'paris', 'libraire', 'littéraire', 'littérature', 'auteurs', 'ouvrages', 'catalogue', 'pierre', 'livres', 'bibliothèque', 'livre', 'fonds']\n",
            "ENCPOS_1996_31\t14\t Emmanuel Rousseau date: 1996 title: Les sceaux des cinq premières maisons de l’ordre de Citeaux (1098-1516)    \t['médiévale', 'table', 'abbé', 'inventaires', 'pontigny', 'lettres', 'saint', 'abbaye', 'cisterciennes', 'église', 'bibliothèque', 'clairvaux', 'bernard', 'abbés']\n",
            "ENCPOS_1962_02\t14\t Annie Angremy date: 1962 title: Les œuvres poétiques de Pierre de Beauvais    \t['textes', 'table', 'manuscrits', 'manuscrit', 'œuvres', 'littérature', 'ouvrages', 'saint', 'moine', 'abbaye', 'pierre', 'bibliothèque', 'bibliques', 'siècle']\n",
            "ENCPOS_1953_17\t14\t Geneviève Nortier-Marchand date: 1953 title: Les bibliothèques médiévales des abbayes bénédictines de Normandie    \t['abbé', 'manuscrits', 'bibliothèques', 'liturgiques', 'ouvrages', 'catalogue', 'saint', 'abbaye', 'livres', 'bibliothèque', 'liste', 'siècle', 'abbés', 'documents']\n",
            "ENCPOS_1867_10\t14\t Alphonse Vétault date: 1867 title: L’abbaye Saint-Victor de Paris, depuis sa fondation jusqu’au temps de saint Louis, 1108-1229 : histoire, état intérieur, enseignement de son école    \t['abbé', 'auteur', 'œuvres', 'littérature', 'ouvrages', 'saint', 'abbaye', 'livres', 'bibliothèque', 'liste', 'bernard', 'siècle', 'abbés', 'documents']\n"
          ]
        }
      ],
      "source": [
        "# Search by docs, using shared keywords\n",
        "cherche=set(keywords_by_doc[test_doc_id].split()) # On dédoublonne ou pas ?\n",
        "keywords_by_doc_tokenized = {}\n",
        "related_docs = []\n",
        "\n",
        "for doc_id in keywords_by_doc.keys():\n",
        "  keywords_by_doc_tokenized[doc_id] = set(keywords_by_doc[doc_id].split())\n",
        "  try:\n",
        "    a=[x for x in cherche if x in keywords_by_doc_tokenized[doc_id]]\n",
        "    related_docs.append([doc_id, len(a), docs_structured[doc_id][\"metadata\"].split(\"creator:\")[1], a])\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "related_docs=sorted(related_docs, key = lambda x: x[1], reverse=True)[:20]   \n",
        "for x in related_docs:\n",
        "  if test_doc_id!=x[0]:\n",
        "    print(*x, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_2zBfMeOgH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a799195a-418d-452b-ca0f-3fe44bdc2603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENCPOS_2013_16\t25\t Guy Mayaud date: 2013 title: L’érudition héraldique au xviie siècle : la question des origines des armoiries    \t['étude', 'héraldique', 'bibliothèque', 'armoiries', 'armoiries', 'objets', 'monuments', 'armoiries', 'armoiries', 'médiévale', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'héraldiques', 'armes', 'armoiries', 'armoiries', 'étude', 'héraldique']\n",
            "ENCPOS_1997_37\t25\t Nicolas Roche date: 1997 title: Les armoiries imaginaires des personnages de l’Antiquité, de l’Orient et de la Bible (xiie-xviie siècle)    \t['héraldique', 'armoiries', 'armoriaux', 'armoiries', 'armoriaux', 'armoiries', 'lion', 'armoiries', 'armoiries', 'armoiries', 'lion', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'lion', 'armes', 'armoiries', 'symboliques', 'armes', 'armoiries', 'armoiries', 'héraldique', 'historien']\n",
            "ENCPOS_2020_06\t23\t Élisabeth Charron date: 2020 title: Le lion et la couronne. Les Estouteville, le roi et le royaume de France, vers 1350-vers 1480    \t['bibliothèque', 'archives', 'armoiries', 'armoiries', 'emblèmes', 'armoiries', 'lion', 'armoiries', 'armoiries', 'armoiries', 'lion', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'lion', 'armes', 'armoiries', 'source', 'armes', 'armoiries', 'armoiries']\n",
            "ENCPOS_2010_27\t23\t Caroline Vrand date: 2010 title: Les collections d’objets d’art d’Anne de Bretagne à travers ses inventaires. Le spectacle et les coulisses.    \t['étude', 'collections', 'bibliothèque', 'archives', 'armoiries', 'armoiries', 'objets', 'emblèmes', 'armoiries', 'armoiries', 'meubles', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'ensemble', 'source', 'meubles', 'armoiries', 'armoiries', 'étude']\n",
            "ENCPOS_1995_04\t22\t Édouard Bouyé date: 1995 title: Héraldique médiévale des évêques et de cardinaux français : l’exemple de quarante-six diocèses de la France du Nord et du Centre    \t['héraldique', 'généalogique', 'armoiries', 'armoriaux', 'armoiries', 'armoriaux', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'source', 'armes', 'armoiries', 'armoiries', 'héraldique']\n",
            "ENCPOS_2019_08\t22\t Lou Delaveau date: 2019 title: Le revers de l’écaille. La sirène, entre nature et lecture, dans le livre imprimé à l’époque moderne (1475-1691/1692)    \t['étude', 'bibliothèque', 'animaux', 'médiévale', 'animal', 'animaux', 'animal', 'aquatique', 'poisson', 'oiseaux', 'animaux', 'monstres', 'animaux', 'animal', 'source', 'animal', 'animaux', 'animal', 'zoologique', 'animal', 'animaux', 'étude']\n",
            "ENCPOS_1995_03\t22\t Jocelyn Bouquillard date: 1995 title: Le comte Auguste de Bastard (1792-1883), archéologue et imprimeur lithographe    \t['généalogique', 'bibliothèque', 'archives', 'médiévales', 'monuments', 'arts', 'animaux', 'médiévale', 'animaux', 'médiévales', 'médiévales', 'bestiaire', 'animaux', 'bestiaire', 'animaux', 'arts', 'animaux', 'symbolique', 'médiévales', 'animaux', 'bestiaire', 'archéologue']\n",
            "ENCPOS_1942_06\t21\t Madeleine Connat date: 1942 title: Étude sur les inventaires après décès de Paris (1500-1560)    \t['collections', 'bibliothèque', 'armoiries', 'armoiries', 'objets', 'armoiries', 'armoiries', 'meubles', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'meubles', 'armes', 'armoiries', 'armoiries']\n",
            "ENCPOS_1986_07\t21\t Thibaut Girard date: 1986 title: Le Traité du droit et du comportement des armes de Du Cange. Édition critique et commentaire    \t['bibliothèque', 'armoiries', 'armoriaux', 'armoiries', 'armoriaux', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'original', 'héraldiques', 'armes', 'armoiries', 'armoiries']\n",
            "ENCPOS_1994_20\t21\t Fabien Plazannet date: 1994 title: Les sceaux gascons du Moyen Âge    \t['bibliothèque', 'archives', 'armoiries', 'armoriaux', 'armoiries', 'armoriaux', 'armoiries', 'armoiries', 'géographique', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armoiries']\n",
            "ENCPOS_1990_02\t21\t Laurence Bobis date: 1990 title: Contribution à l’histoire du chat dans l’occident médiéval : étude critique des sources    \t['étude', 'récits', 'monuments', 'origine', 'animaux', 'animal', 'animaux', 'animal', 'bestiaire', 'animaux', 'bestiaire', 'animaux', 'animal', 'animal', 'animaux', 'animal', 'symboliques', 'animal', 'animaux', 'étude', 'bestiaire']\n",
            "ENCPOS_1999_21\t20\t Laurent Ferri date: 1999 title: L’héraldique balzacienne, mise en perspective    \t['choix', 'héraldique', 'archives', 'armoiries', 'armoriaux', 'armoiries', 'armoriaux', 'objets', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'héraldiques', 'armoiries', 'armoiries', 'héraldique']\n",
            "ENCPOS_1999_04\t20\t Mathias Auclair date: 1999 title: Politique lignagère et ambitions comtales en Lorraine : la famille et la seigneurie d’Apremont, des origines au début du xive siècle    \t['bibliothèque', 'archives', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'bar', 'source', 'armes', 'armoiries', 'armoiries']\n",
            "ENCPOS_1944_12\t20\t Rémi Mathieu date: 1944 title: Le droit héraldique en France jusqu’au milieu du xviie siècle    \t['choix', 'héraldique', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'interdit', 'armes', 'armoiries', 'armoiries', 'héraldique']\n",
            "ENCPOS_1867_07\t20\t Étienne Le Grand date: 1867 title: Essai sur l’office d’armes au Moyen Âge : rois, maréchaux, hérauts, poursuivants d’armes    \t['héraldique', 'armoiries', 'armoiries', 'origine', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'source', 'armes', 'armoiries', 'armoiries', 'héraldique']\n",
            "ENCPOS_1968_02\t20\t Anne Cavignac-Adrien date: 1968 title: Les noms de lieu du canton de Blanquefort (Gironde)    \t['archives', 'médiévales', 'origine', 'germanique', 'animaux', 'géographique', 'animal', 'animaux', 'animal', 'médiévales', 'médiévales', 'animaux', 'animaux', 'animal', 'animal', 'animaux', 'animal', 'médiévales', 'animal', 'animaux']\n",
            "ENCPOS_1997_07\t19\t Marianne Besseyre date: 1997 title: L’iconographie de l’arche de Noé, du iiie au xve siècle : du texte aux images    \t['bibliothèque', 'médiévales', 'europe', 'animaux', 'animal', 'animaux', 'animal', 'médiévales', 'médiévales', 'animaux', 'animaux', 'animal', 'animal', 'animaux', 'animal', 'symbolique', 'médiévales', 'animal', 'animaux']\n",
            "ENCPOS_1989_12\t18\t Benoît Jordan date: 1989 title: Les seigneurs de Ribeaupierre (1451-1585)    \t['bibliothèque', 'archives', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armes', 'armoiries', 'armoiries']\n",
            "ENCPOS_2008_10\t18\t Nathalie Daigne date: 2008 title: Le livre de raison d’Honoré de Quiqueran de Beaujeu    \t['étude', 'bibliothèque', 'archives', 'armoiries', 'armoiries', 'monuments', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'armoiries', 'étude', 'historien']\n"
          ]
        }
      ],
      "source": [
        "# Search by docs, using shared keywords // IDEM SANS DÉDOUBLONNER (DISCUTER AVEC ST)\n",
        "cherche=keywords_by_doc[test_doc_id].split()\n",
        "keywords_by_doc_tokenized = {}\n",
        "related_docs = []\n",
        "\n",
        "for doc_id in keywords_by_doc.keys():\n",
        "  keywords_by_doc_tokenized[doc_id] = keywords_by_doc[doc_id].split()\n",
        "  try:\n",
        "    a=[x for x in cherche if x in keywords_by_doc_tokenized[doc_id]]\n",
        "    related_docs.append([doc_id, len(a), docs_structured[doc_id][\"metadata\"].split(\"creator:\")[1], a])\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "related_docs=sorted(related_docs, key = lambda x: x[1], reverse=True)[:20]   \n",
        "for x in related_docs:\n",
        "  if test_doc_id!=x[0]:\n",
        "    print(*x, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjYV_ObXAto8"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keywords similarities matrix"
      ],
      "metadata": {
        "id": "ooXwzKfew2TF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bC63umH0V5k",
        "outputId": "cab52ee3-03aa-4aaa-d75a-b7b0898f00e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n"
          ]
        }
      ],
      "source": [
        "# Calculate keywords similarities matrix 3000 X 3000\n",
        "\n",
        "matrix_keywords={}\n",
        "liste_encpos=sorted(list(docs_structured.keys())) # trier pour que les matrices aient la même structure\n",
        "\n",
        "for i, pos in enumerate(liste_encpos):\n",
        "  matrix_keywords[pos]=[]\n",
        "  for pos_b in liste_encpos:\n",
        "    if pos!=pos_b:\n",
        "      matrix_keywords[pos].append([pos_b, cos_similarity(keywords_vectors[pos], keywords_vectors[pos_b])])\n",
        "  if i%500==0:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export keywords_similarities_matrix\n",
        "import json\n",
        "with open('keywords_similarities_matrix.json', 'w', encoding='utf8') as f:\n",
        "    json.dump(matrix_keywords, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "hh7d7CxwrZB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document similarities matrix"
      ],
      "metadata": {
        "id": "LyteXy1sw8dh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJawL141zKJB",
        "outputId": "a35e2e37-8973-4447-fb41-aa1f687ad878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n"
          ]
        }
      ],
      "source": [
        "# Calculate document similarities matrix 3000x3000\n",
        "\n",
        "matrix_docs={}\n",
        "liste_encpos=sorted(list(docs_structured.keys()))\n",
        "\n",
        "for i, pos in enumerate(liste_encpos):\n",
        "  matrix_docs[pos]=[]\n",
        "  for pos_b in liste_encpos:\n",
        "    if pos!=pos_b:\n",
        "          matrix_docs[pos].append([pos_b, cos_similarity(document_vectors[pos], document_vectors[pos_b])])\n",
        "  if i%500==0:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvZPIGGYzOaY"
      },
      "outputs": [],
      "source": [
        "# Export document_similarities_matrix\n",
        "import json\n",
        "with open('document_similarities_matrix.json', 'w', encoding='utf8') as f:\n",
        "    json.dump(matrix_docs, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec similarities matrix"
      ],
      "metadata": {
        "id": "zNygVoGNzRly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Doc2Vec similarities matrix 3000x3000 // TEST\n",
        "\n",
        "matrix_d2v={}\n",
        "liste_encpos=sorted(list(docs_structured.keys()))\n",
        "\n",
        "for i, pos in enumerate(liste_encpos):\n",
        "  matrix_d2v[pos]=[]\n",
        "  for pos_b in liste_encpos:\n",
        "    if pos!=pos_b:\n",
        "      coeff=cos_similarity(d2v_model.docvecs[pos].reshape(1, -1), d2v_model.docvecs[pos_b].reshape(1, -1))\n",
        "      matrix_d2v[pos].append([pos_b, coeff])\n",
        "  if i%500==0:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYpXDKdBC73W",
        "outputId": "2d736f1f-0105-4333-c315-0d3c4de5707c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('doc2vec_similarities_matrix.json', 'w', encoding='utf8') as f:\n",
        "    json.dump(matrix_d2v, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "PqVYTV7O3jZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2v_model.docvecs"
      ],
      "metadata": {
        "id": "PQWk67Lyb1gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sir56KfD8SM"
      },
      "outputs": [],
      "source": [
        "# Calculate Doc2Vec similarities matrix 3000x3000 // REJET ne fonctionne pas à discuter (debug ci-dessus) / valider avec ST\n",
        "\n",
        "matrix_d2v={}\n",
        "liste_encpos=list(docs_structured.keys())\n",
        "\n",
        "for i, pos in enumerate(liste_encpos):\n",
        "  for pos_b in liste_encpos:\n",
        "    if pos!=pos_b:\n",
        "      coeff=cos_similarity(d2v_model.docvecs[pos].reshape(1, -1), d2v_model.docvecs[pos_b].reshape(1, -1))\n",
        "      for ii, x in enumerate(matrix_d2v[pos]): # ICI, POURQUOI ?\n",
        "        if pos_b in x[0]:\n",
        "          matrix_d2v[pos][ii]=matrix_d2v[pos][ii]+[coeff]\n",
        "  if i%500==0:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN8D74wwHb9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aecd6ee-d689-434d-cceb-685cf35a2329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1885162740945816"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ],
      "source": [
        "# test\n",
        "cos_similarity(model_d2v.docvecs[\"ENCPOS_2000_10\"].reshape(1, -1), model_d2v.docvecs[\"ENCPOS_2000_09\"].reshape(1, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mesure de la déviation entre les scores de similarité\n",
        "\n",
        "all vs all scheme"
      ],
      "metadata": {
        "id": "YvMgk4Vx31Fu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PcRHnL8lzVy4"
      },
      "outputs": [],
      "source": [
        "#all vs all comparison, we get a list with the absolute difference between each two similarity scores (all vs all) calculated for each method (keywords, documents and doc2vec)\n",
        "from statistics import mean, median, variance, stdev\n",
        "\n",
        "def similarities_methods_comparison(matrix_a, matrix_b):\n",
        "  all2all = lambda a, b : [abs(item[1]-b[k][i][1]) for k,v in a.items() for i,item in enumerate(v)] \n",
        "  all2all = all2all(matrix_a, matrix_b) #pass two matrix dictionaries as args\n",
        "  print('mean: {0:.3f}'.format(mean(all2all)))\n",
        "  print('median: {0:.3f}'.format(median(all2all)))\n",
        "  print('var: {0:.3f}'.format(variance(all2all)))\n",
        "  print('stdev: {0:.3f}'.format(stdev(all2all)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/chartes/encpos_similarities/raw/master/evaluation/keywords_similarities_matrix.json\n",
        "keywords_similarities_matrix = {}\n",
        "with open('/content/keywords_similarities_matrix.json', 'r') as json_file:\n",
        "  keywords_similarities_matrix = json.load(json_file)"
      ],
      "metadata": {
        "id": "x5nY0GujcXdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/chartes/encpos_similarities/raw/master/evaluation/document_similarities_matrix.json\n",
        "document_similarities_matrix = {}\n",
        "with open('/content/document_similarities_matrix.json', 'r') as json_file:\n",
        "  document_similarities_matrix = json.load(json_file)"
      ],
      "metadata": {
        "id": "f-w4nJYw6ImY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/chartes/encpos_similarities/raw/master/evaluation/doc2vec_similarities_matrix.json\n",
        "doc2vec_similarities_matrix = {}\n",
        "with open('/content/doc2vec_similarities_matrix.json', 'r') as json_file:\n",
        "  doc2vec_similarities_matrix = json.load(json_file)"
      ],
      "metadata": {
        "id": "Y9yyrRqeGqwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keywords vs doc2vec\n",
        "print('keywords vs doc2vec')\n",
        "similarities_methods_comparison(keywords_similarities_matrix,\n",
        "                                doc2vec_similarities_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuvVWA9bF0nT",
        "outputId": "42652337-7ceb-4e62-e41f-e8c979fb6174"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keywords vs doc2vec\n",
            "mean: 0.177\n",
            "median: 0.154\n",
            "var: 0.017\n",
            "stdev: 0.129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# document (distilbert) vs doc2vec\n",
        "print('document (distilbert) vs doc2vec')\n",
        "similarities_methods_comparison(document_similarities_matrix,\n",
        "                                doc2vec_similarities_matrix)"
      ],
      "metadata": {
        "id": "ew7Inx3nIvzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bb1396-918a-461d-ebc9-b74182398395"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document (distilbert) vs doc2vec\n",
            "mean: 0.163\n",
            "median: 0.141\n",
            "var: 0.014\n",
            "stdev: 0.119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keywords vs document (distilbert)\n",
        "print('keywords vs document (distilbert)')\n",
        "similarities_methods_comparison(keywords_similarities_matrix,\n",
        "                                document_similarities_matrix)"
      ],
      "metadata": {
        "id": "nma7q7wTIHNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69825f75-ecc2-425a-8818-b155cae15fc9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keywords vs document (distilbert)\n",
            "mean: 0.091\n",
            "median: 0.076\n",
            "var: 0.005\n",
            "stdev: 0.070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihgfwy0cLdY8"
      },
      "source": [
        "# Rejets / mémo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhgvryOUAIfW"
      },
      "source": [
        "### entity linking wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV16nX-oZx4T"
      },
      "outputs": [],
      "source": [
        "# SKIP\n",
        "#entity linking by using french and english wikipedia \n",
        "for node in nodes:\n",
        "  try:\n",
        "    wikipedia.set_lang(\"fr\")\n",
        "    x=wikipedia.page(node, auto_suggest=False)\n",
        "    print(node, x.url)\n",
        "  except (wikipedia.exceptions.DisambiguationError) as e:\n",
        "    try:\n",
        "      options=e.options\n",
        "      #x=[y for y in x if any(z in wikipedia.summary(y, auto_suggest=False) for z in nodes)]\n",
        "      print(options)\n",
        "    except:\n",
        "      continue\n",
        "  except wikipedia.PageError:\n",
        "    wikipedia.set_lang(\"en\")\n",
        "    try:\n",
        "      x=wikipedia.page(node, auto_suggest=False)\n",
        "      print(node, x.url)\n",
        "    except (wikipedia.exceptions.DisambiguationError, wikipedia.PageError) as e:\n",
        "      try:\n",
        "        options=e.options\n",
        "        #x=[y for y in x if any(z in wikipedia.summary(y, auto_suggest=False) for z in nodes)]\n",
        "        print(options)\n",
        "      except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_structured[test_doc_id]['metadata']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IzFt_CRvcWGY",
        "outputId": "c1be24f2-d0a5-4a29-b3b5-a9fd12d08e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'identifier: ENCPOS_1972_18 creator: Michel Pastoureau date: 1972 title: Le bestiaire héraldique au Moyen Âge    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUQv6zy37njk",
        "outputId": "375d2245-0056-4640-96e8-d9ad6fb4090f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14194782078266144"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "cos_similarity(document_vectors[\"ENCPOS_2011_08\"], document_vectors[\"ENCPOS_1988_10\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST (SUPPRIMER) /// Similarity using keywords lists\n",
        "cherche=keywords_by_doc[test_doc_id]\n",
        "similar_docs_number = 10\n",
        "similar_docs = []\n",
        "\n",
        "for doc_id in docs_structured.keys():\n",
        "  try:\n",
        "    coeff=(2*len(list(set(cherche) & set(keywords_by_doc[doc_id]))))/(len(cherche)+len(keywords_by_doc[doc_id]))\n",
        "    if test_doc_id == doc_id:\n",
        "      continue\n",
        "    if coeff>0.05:\n",
        "      similar_docs.append([coeff, doc_id, keywords_by_doc[doc_id]])\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "similar_docs.sort(key=lambda x:x[0], reverse=True)\n",
        "#similar_docs[0:similar_docs_number]\n",
        "for l in similar_docs[0:similar_docs_number]:\n",
        "  print(*l[0:3])"
      ],
      "metadata": {
        "id": "E8T2xZs9v9Qj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUPPRIMER ? Sergio\n",
        "cherche=model.encode([\"animal\"])\n",
        "related_docs = []\n",
        "\n",
        "for k,v in document_vectors.items():\n",
        "  cos=cosine_similarity(cherche, v)\n",
        "  if cos>0.1:\n",
        "    # issues to fix with docs metadata.\n",
        "    try:\n",
        "      print(k, \"\\t\", cos[0], \"\\t\", docs_structured[k][\"metadata\"].split(\"creator:\")[1])\n",
        "    except:\n",
        "      continue"
      ],
      "metadata": {
        "id": "a4NkzNvhjlS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "encpos_similarities.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}